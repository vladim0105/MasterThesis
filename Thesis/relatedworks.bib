@unpublished{BAMI,
   title={Biological and Machine Intelligence (BAMI)},
   author={Hawkins, J. and Ahmad, S. and Purdy, S. and Lavin, A.},
   note={Initial online release 0.4},
   url={https://numenta.com/resources/biological-and-machine-intelligence/},
   year={2016}
}

@article{AHMAD2017134,
    title = "Unsupervised real-time anomaly detection for streaming data",
    journal = "Neurocomputing",
    volume = "262",
    pages = "134 - 147",
    year = "2017",
    note = "Online Real-Time Learning Strategies for Data Streams",
    issn = "0925-2312",
    doi = "https://doi.org/10.1016/j.neucom.2017.04.070",
    url = "http://www.sciencedirect.com/science/article/pii/S0925231217309864",
    author = "Subutai Ahmad and Alexander Lavin and Scott Purdy and Zuha Agha",
    keywords = "Anomaly detection, Hierarchical Temporal Memory, Streaming data, Unsupervised learning, Concept drift, Benchmark dataset",
    abstract = "We are seeing an enormous increase in the availability of streaming, time-series data. Largely driven by the rise of connected real-time data sources, this data presents technical challenges and opportunities. One fundamental capability for streaming analytics is to model each stream in an unsupervised fashion and detect unusual, anomalous behaviors in real-time. Early anomaly detection is valuable, yet it can be difficult to execute reliably in practice. Application constraints require systems to process data in real-time, not batches. Streaming data inherently exhibits concept drift, favoring algorithms that learn continuously. Furthermore, the massive number of independent streams in practice requires that anomaly detectors be fully automated. In this paper we propose a novel anomaly detection algorithm that meets these constraints. The technique is based on an online sequence memory algorithm called Hierarchical Temporal Memory (HTM). We also present results using the Numenta Anomaly Benchmark (NAB), a benchmark containing real-world data streams with labeled anomalies. The benchmark, the first of its kind, provides a controlled open-source environment for testing anomaly detection algorithms on streaming data. We present results and analysis for a wide range of algorithms on this benchmark, and discuss future challenges for the emerging field of streaming analytics."
}
@inbook{ObjectDetectionSIFT,
author = {Fallas-Moya, Fabian and Torres-Rojas, Francisco J.},
year = {2018},
month = {01},
pages = {1-14},
title = {Object Recognition Using Hierarchical Temporal Memory},
isbn = {978-3-319-76260-9},
doi = {10.1007/978-3-319-76261-6_1}
}
@misc{HTMEncoding,
    title={Encoding Data for HTM Systems}, 
    author={Scott Purdy},
    year={2016},
    eprint={1602.05925},
    archivePrefix={arXiv},
    primaryClass={cs.NE}
}

@inbook{MotionAnomalyDetection,
    author = {Daylidyonok, Ilya and Frolenkova, Anastasiya and Panov, Aleksandr},
    year = {2019},
    month = {01},
    pages = {69-81},
    title = {Extended Hierarchical Temporal Memory for Motion Anomaly Detection: Proceedings of the Ninth Annual Meeting of the BICA Society},
    isbn = {978-3-319-99315-7},
    doi = {10.1007/978-3-319-99316-4_10}
}

@misc{anomalyvideo,
    title={Video Anomaly Detection for Smart Surveillance}, 
    author={Sijie Zhu and Chen Chen and Waqas Sultani},
    year={2020},
    eprint={2004.00222},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{eyeencoder,
    author={David McDougall (ctrl-z-9000-times)},
    year={2019},
    month={9},
    day={20},
    url={https://github.com/htm-community/htm.core/issues/259#issuecomment-533333336}
}
@misc{htm_predictions_count,
    author={Sam Heiserman (sheiser1)},
    year={2022},
    month={1},
    day={5},
    url={https://discourse.numenta.org/t/htm-core-am-i-getting-prediction-density-correctly/9299}
}
@misc{cortical_region,
    author={MRaptor},
    year={2016},
    month={6},
    url={https://discourse.numenta.org/t/htm-cheat-sheet/828}
}
@INPROCEEDINGS{CNN_HTM,
  author={Y. {Zou} and Y. {Shi} and Y. {Wang} and Y. {Shu} and Q. {Yuan} and Y. {Tian}},
  booktitle={2018 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={Hierarchical Temporal Memory Enhanced One-Shot Distance Learning for Action Recognition}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/ICME.2018.8486447}}
  
@ARTICLE{thousandbrains,
AUTHOR={Hawkins, Jeff and Lewis, Marcus and Klukas, Mirko and Purdy, Scott and Ahmad, Subutai},   
TITLE={A Framework for Intelligence and Cortical Function Based on Grid Cells in the Neocortex},      
JOURNAL={Frontiers in Neural Circuits},      
VOLUME={12},      
PAGES={121},     
YEAR={2019},      
URL={https://www.frontiersin.org/article/10.3389/fncir.2018.00121},       
DOI={10.3389/fncir.2018.00121},      
ISSN={1662-5110},   
ABSTRACT={How the neocortex works is a mystery. In this paper we propose a novel framework for understanding its function. Grid cells are neurons in the entorhinal cortex that represent the location of an animal in its environment. Recent evidence suggests that grid cell-like neurons may also be present in the neocortex. We propose that grid cells exist throughout the neocortex, in every region and in every cortical column. They define a location-based framework for how the neocortex functions. Whereas grid cells in the entorhinal cortex represent the location of one thing, the body relative to its environment, we propose that cortical grid cells simultaneously represent the location of many things. Cortical columns in somatosensory cortex track the location of tactile features relative to the object being touched and cortical columns in visual cortex track the location of visual features relative to the object being viewed. We propose that mechanisms in the entorhinal cortex and hippocampus that evolved for learning the structure of environments are now used by the neocortex to learn the structure of objects. Having a representation of location in each cortical column suggests mechanisms for how the neocortex represents object compositionality and object behaviors. It leads to the hypothesis that every part of the neocortex learns complete models of objects and that there are many models of each object distributed throughout the neocortex. The similarity of circuitry observed in all cortical regions is strong evidence that even high-level cognitive tasks are learned and represented in a location-based framework.}
}


@misc{fpn,
      title={Feature Pyramid Networks for Object Detection}, 
      author={Tsung-Yi Lin and Piotr Doll√°r and Ross Girshick and Kaiming He and Bharath Hariharan and Serge Belongie},
      year={2017},
      eprint={1612.03144},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{inceptionnet,
      title={Going Deeper with Convolutions}, 
      author={Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
      year={2014},
      eprint={1409.4842},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@ARTICLE{htm_neurons,
  
AUTHOR={Hawkins, Jeff and Ahmad, Subutai},   
	 
TITLE={Why Neurons Have Thousands of Synapses, a Theory of Sequence Memory in Neocortex},      
	
JOURNAL={Frontiers in Neural Circuits},      
	
VOLUME={10},      

PAGES={23},     
	
YEAR={2016},      
	  
URL={https://www.frontiersin.org/article/10.3389/fncir.2016.00023},       
	
DOI={10.3389/fncir.2016.00023},      
	
ISSN={1662-5110},   
   
ABSTRACT={Pyramidal neurons represent the majority of excitatory neurons in the neocortex. Each pyramidal neuron receives input from thousands of excitatory synapses that are segregated onto dendritic branches. The dendrites themselves are segregated into apical, basal, and proximal integration zones, which have different properties. It is a mystery how pyramidal neurons integrate the input from thousands of synapses, what role the different dendrites play in this integration, and what kind of network behavior this enables in cortical tissue. It has been previously proposed that non-linear properties of dendrites enable cortical neurons to recognize multiple independent patterns. In this paper we extend this idea in multiple ways. First we show that a neuron with several thousand synapses segregated on active dendrites can recognize hundreds of independent patterns of cellular activity even in the presence of large amounts of noise and pattern variation. We then propose a neuron model where patterns detected on proximal dendrites lead to action potentials, defining the classic receptive field of the neuron, and patterns detected on basal and apical dendrites act as predictions by slightly depolarizing the neuron without generating an action potential. By this mechanism, a neuron can predict its activation in hundreds of independent contexts. We then present a network model based on neurons with these properties that learns time-based sequences. The network relies on fast local inhibition to preferentially activate neurons that are slightly depolarized. Through simulation we show that the network scales well and operates robustly over a wide range of parameters as long as the network uses a sparse distributed code of cellular activations. We contrast the properties of the new network model with several other neural network models to illustrate the relative capabilities of each. We conclude that pyramidal neurons with thousands of synapses, active dendrites, and multiple integration zones create a robust and powerful sequence memory. Given the prevalence and similarity of excitatory neurons throughout the neocortex and the importance of sequence memory in inference and behavior, we propose that this form of sequence memory may be a universal property of neocortical tissue.}
}

@ARTICLE{cnn_invariance,  author={Y. {Lecun} and L. {Bottou} and Y. {Bengio} and P. {Haffner}},  journal={Proceedings of the IEEE},   title={Gradient-based learning applied to document recognition},   year={1998},  volume={86},  number={11},  pages={2278-2324},  doi={10.1109/5.726791}}

@article{anomaly_detection,
   title={Deep Learning for Anomaly Detection},
   volume={54},
   ISSN={1557-7341},
   url={http://dx.doi.org/10.1145/3439950},
   DOI={10.1145/3439950},
   number={2},
   journal={ACM Computing Surveys},
   publisher={Association for Computing Machinery (ACM)},
   author={Pang, Guansong and Shen, Chunhua and Cao, Longbing and Hengel, Anton Van Den},
   year={2021},
   month={4},
   pages={1‚Äì38}
}
@misc{ganomaly,
      title={GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training}, 
      author={Samet Akcay and Amir Atapour-Abarghouei and Toby P. Breckon},
      year={2018},
      eprint={1805.06725},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{htm_zeta1,
  title={Hierarchical Temporal Memory Concepts , Theory , and Terminology},
  author={Jeff Hawkins and Dileep George},
  year={2006}
}