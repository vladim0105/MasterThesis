
@article{perceptron,
  title   = {The perceptron: a probabilistic model for information storage and organization in the brain.},
  author  = {Frank Rosenblatt},
  journal = {Psychological review},
  year    = {1958},
  volume  = {65 6},
  pages   = {
             386-408
             }
}
@article{perceptron2,
  author  = {Freund, Yoav and Schapire, Robert},
  year    = {1999},
  month   = {02},
  pages   = {},
  title   = {Large Margin Classification Using the Perceptron Algorithm},
  volume  = {37},
  journal = {Machine Learning},
  doi     = {10.1023/A:1007662407062}
}
@article{perceptron3,
  title     = {Perceptrons.},
  author    = {Minsky, Marvin and Papert, Seymour},
  year      = {1969},
  publisher = {MIT press}
}
@article{perceptron_misconceptions,
  author  = {Mikel Olazaran},
  title   = {A Sociological Study of the Official History of the Perceptrons Controversy},
  journal = {Social Studies of Science},
  volume  = {26},
  number  = {3},
  pages   = {611-659},
  year    = {1996},
  doi     = {10.1177/030631296026003005},
  url     = { 
             https://doi.org/10.1177/030631296026003005
             
             },
  eprint  = { 
             https://doi.org/10.1177/030631296026003005
             
             }
}
@article{backprop,
  title     = {Learning representations by back-propagating errors},
  author    = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal   = {nature},
  volume    = {323},
  number    = {6088},
  pages     = {533--536},
  year      = {1986},
  publisher = {Nature Publishing Group}
}
@misc{relu,
  title         = {Deep Learning using Rectified Linear Units (ReLU)},
  author        = {Abien Fred Agarap},
  year          = {2019},
  eprint        = {1803.08375},
  archiveprefix = {arXiv},
  primaryclass  = {cs.NE}
}
@article{sgd,
  title   = {An overview of gradient descent optimization algorithms},
  author  = {Ruder, Sebastian},
  journal = {arXiv preprint arXiv:1609.04747},
  year    = {2016}
}
@article{gradient_descent,
  author  = {Carpenter, Kristy and Cohen, David and Jarrell, Juliet and Huang, Xudong},
  year    = {2018},
  month   = {10},
  pages   = {},
  title   = {Deep learning and virtual drug screening},
  volume  = {10},
  journal = {Future Medicinal Chemistry},
  doi     = {10.4155/fmc-2018-0314}
}
@inproceedings{svm,
  title     = {A training algorithm for optimal margin classifiers},
  author    = {Boser, Bernhard E and Guyon, Isabelle M and Vapnik, Vladimir N},
  booktitle = {Proceedings of the fifth annual workshop on Computational learning theory},
  pages     = {144--152},
  year      = {1992}
}
@article{dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}
@inproceedings{batchnorm,
  title        = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author       = {Ioffe, Sergey and Szegedy, Christian},
  booktitle    = {International conference on machine learning},
  pages        = {448--456},
  year         = {2015},
  organization = {PMLR}
}
@article{data_augmentation,
  title     = {A survey on image data augmentation for deep learning},
  author    = {Shorten, Connor and Khoshgoftaar, Taghi M},
  journal   = {Journal of big data},
  volume    = {6},
  number    = {1},
  pages     = {1--48},
  year      = {2019},
  publisher = {Springer}
}
@inproceedings{autoencoder,
  title     = {Modular learning in neural networks.},
  author    = {Ballard, Dana H},
  booktitle = {Aaai},
  volume    = {647},
  pages     = {279--284},
  year      = {1987}
}
@article{lstm,
  author  = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  year    = {1997},
  month   = {12},
  pages   = {1735-80},
  title   = {Long Short-term Memory},
  volume  = {9},
  journal = {Neural computation},
  doi     = {10.1162/neco.1997.9.8.1735}
}
@misc{gru,
  title         = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling},
  author        = {Junyoung Chung and Caglar Gulcehre and KyungHyun Cho and Yoshua Bengio},
  year          = {2014},
  eprint        = {1412.3555},
  archiveprefix = {arXiv},
  primaryclass  = {cs.NE}
}
@misc{attention,
  title         = {Neural Machine Translation by Jointly Learning to Align and Translate},
  author        = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  year          = {2016},
  eprint        = {1409.0473},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@misc{transformer,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2017},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@article{cnn,
  author  = {Y. {Lecun} and L. {Bottou} and Y. {Bengio} and P. {Haffner}},
  journal = {Proceedings of the IEEE},
  title   = {Gradient-based learning applied to document recognition},
  year    = {1998},
  volume  = {86},
  number  = {11},
  pages   = {2278-2324},
  doi     = {10.1109/5.726791}
}
@article{alexnet,
  title   = {Imagenet classification with deep convolutional neural networks},
  author  = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal = {Advances in neural information processing systems},
  volume  = {25},
  year    = {2012}
}
@inproceedings{deeplearning_gpu,
  title     = {Large-scale deep unsupervised learning using graphics processors},
  author    = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y},
  booktitle = {Proceedings of the 26th annual international conference on machine learning},
  pages     = {873--880},
  year      = {2009}
}
@article{pytorch,
  title   = {Pytorch: An imperative style, high-performance deep learning library},
  author  = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal = {Advances in neural information processing systems},
  volume  = {32},
  year    = {2019}
}
@article{tensorflow,
  title   = {Tensorflow: Large-scale machine learning on heterogeneous distributed systems},
  author  = {Abadi, Martin and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and others},
  journal = {arXiv preprint arXiv:1603.04467},
  year    = {2016}
}
@misc{gan,
  title         = {Generative Adversarial Networks},
  author        = {Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
  year          = {2014},
  eprint        = {1406.2661},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}
@misc{vae,
  title         = {Auto-Encoding Variational Bayes},
  author        = {Diederik P Kingma and Max Welling},
  year          = {2014},
  eprint        = {1312.6114},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML}
}
@misc{gan_challenges,
  doi       = {10.48550/ARXIV.2005.00065},
  url       = {https://arxiv.org/abs/2005.00065},
  author    = {Saxena, Divya and Cao, Jiannong},
  title     = {Generative Adversarial Networks (GANs): Challenges, Solutions, and Future Directions},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{gan_challenges2,
  author  = {Chen, Haiyang},
  year    = {2021},
  month   = {03},
  pages   = {012066},
  title   = {Challenges and Corresponding Solutions of Generative Adversarial Networks (GANs): A Survey Study},
  volume  = {1827},
  journal = {Journal of Physics: Conference Series},
  doi     = {10.1088/1742-6596/1827/1/012066}
}
@misc{wgan,
  doi       = {10.48550/ARXIV.1701.07875},
  url       = {https://arxiv.org/abs/1701.07875},
  author    = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
  keywords  = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Wasserstein GAN},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{unrolled_gan,
  doi       = {10.48550/ARXIV.1611.02163},
  url       = {https://arxiv.org/abs/1611.02163},
  author    = {Metz, Luke and Poole, Ben and Pfau, David and Sohl-Dickstein, Jascha},
  keywords  = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Unrolled Generative Adversarial Networks},
  publisher = {arXiv},
  year      = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{CycleGAN,
  doi       = {10.48550/ARXIV.1703.10593},
  url       = {https://arxiv.org/abs/1703.10593},
  author    = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{cgan,
  doi       = {10.48550/ARXIV.1411.1784},
  url       = {https://arxiv.org/abs/1411.1784},
  author    = {Mirza, Mehdi and Osindero, Simon},
  keywords  = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Conditional Generative Adversarial Nets},
  publisher = {arXiv},
  year      = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{deeplearning_dataset,
  title     = {Revisiting unreasonable effectiveness of data in deep learning era},
  author    = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  pages     = {843--852},
  year      = {2017}
}
@misc{deeplearning_ood,
  author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Sculley, D.},
  year   = {2020},
  month  = {11},
  pages  = {},
  title  = {Underspecification Presents Challenges for Credibility in Modern Machine Learning}
}
@misc{memae,
  doi       = {10.48550/ARXIV.1904.02639},
  url       = {https://arxiv.org/abs/1904.02639},
  author    = {Gong, Dong and Liu, Lingqiao and Le, Vuong and Saha, Budhaditya and Mansour, Moussa Reda and Venkatesh, Svetha and Hengel, Anton van den},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection},
  publisher = {arXiv},
  year      = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{future_frame_prediction,
  doi       = {10.48550/ARXIV.1712.09867},
  url       = {https://arxiv.org/abs/1712.09867},
  author    = {Liu, Wen and Luo, Weixin and Lian, Dongze and Gao, Shenghua},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Future Frame Prediction for Anomaly Detection -- A New Baseline},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{smart_surveillance_2,
  title     = {Intelligent video surveillance: a review through deep learning techniques for crowd analysis},
  author    = {Sreenu, G and Durai, MA Saleem},
  journal   = {Journal of Big Data},
  volume    = {6},
  number    = {1},
  pages     = {1--27},
  year      = {2019},
  publisher = {SpringerOpen}
}
@unpublished{BAMI,
  title  = {Biological and Machine Intelligence (BAMI)},
  author = {Hawkins, J. and Ahmad, S. and Purdy, S. and Lavin, A.},
  note   = {Initial online release 0.4},
  url    = {https://numenta.com/resources/biological-and-machine-intelligence/},
  year   = {2016}
}
@article{htm_l2_l3,
  title     = {Why does the neocortex have layers and columns, a theory of learning the 3d structure of the world},
  author    = {Hawkins, Jeff and Ahmad, Subutai and Cui, Yuwei},
  journal   = {bioRxiv},
  pages     = {162263},
  year      = {2017},
  publisher = {Cold Spring Harbor Laboratory}
}
@article{AHMAD2017134,
  title    = {Unsupervised real-time anomaly detection for streaming data},
  journal  = {Neurocomputing},
  volume   = {262},
  pages    = {134 - 147},
  year     = {2017},
  note     = {Online Real-Time Learning Strategies for Data Streams},
  issn     = {0925-2312},
  doi      = {https://doi.org/10.1016/j.neucom.2017.04.070},
  url      = {http://www.sciencedirect.com/science/article/pii/S0925231217309864},
  author   = {Subutai Ahmad and Alexander Lavin and Scott Purdy and Zuha Agha},
  keywords = {Anomaly detection, Hierarchical Temporal Memory, Streaming data, Unsupervised learning, Concept drift, Benchmark dataset},
  abstract = {We are seeing an enormous increase in the availability of streaming, time-series data. Largely driven by the rise of connected real-time data sources, this data presents technical challenges and opportunities. One fundamental capability for streaming analytics is to model each stream in an unsupervised fashion and detect unusual, anomalous behaviors in real-time. Early anomaly detection is valuable, yet it can be difficult to execute reliably in practice. Application constraints require systems to process data in real-time, not batches. Streaming data inherently exhibits concept drift, favoring algorithms that learn continuously. Furthermore, the massive number of independent streams in practice requires that anomaly detectors be fully automated. In this paper we propose a novel anomaly detection algorithm that meets these constraints. The technique is based on an online sequence memory algorithm called Hierarchical Temporal Memory (HTM). We also present results using the Numenta Anomaly Benchmark (NAB), a benchmark containing real-world data streams with labeled anomalies. The benchmark, the first of its kind, provides a controlled open-source environment for testing anomaly detection algorithms on streaming data. We present results and analysis for a wide range of algorithms on this benchmark, and discuss future challenges for the emerging field of streaming analytics.}
}
@inbook{ObjectDetectionSIFT,
  author = {Fallas-Moya, Fabian and Torres-Rojas, Francisco J.},
  year   = {2018},
  month  = {01},
  pages  = {1-14},
  title  = {Object Recognition Using Hierarchical Temporal Memory},
  isbn   = {978-3-319-76260-9},
  doi    = {10.1007/978-3-319-76261-6_1}
}

@inbook{MotionAnomalyDetection,
  author = {Daylidyonok, Ilya and Frolenkova, Anastasiya and Panov, Aleksandr},
  year   = {2019},
  month  = {01},
  pages  = {69-81},
  title  = {Extended Hierarchical Temporal Memory for Motion Anomaly Detection: Proceedings of the Ninth Annual Meeting of the BICA Society},
  isbn   = {978-3-319-99315-7},
  doi    = {10.1007/978-3-319-99316-4_10}
}

@misc{anomalyvideo,
  title         = {Video Anomaly Detection for Smart Surveillance},
  author        = {Sijie Zhu and Chen Chen and Waqas Sultani},
  year          = {2020},
  eprint        = {2004.00222},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{eyeencoder,
  author = {David McDougall (ctrl-z-9000-times)},
  year   = {2019},
  month  = {9},
  day    = {20},
  url    = {https://github.com/htm-community/htm.core/issues/259#issuecomment-533333336}
}
@misc{htm_predictions_count,
  author = {Sam Heiserman (sheiser1)},
  year   = {2022},
  month  = {1},
  day    = {5},
  url    = {https://discourse.numenta.org/t/htm-core-am-i-getting-prediction-density-correctly/9299}
}
@misc{cortical_region,
  author = {MRaptor},
  year   = {2016},
  month  = {6},
  url    = {https://discourse.numenta.org/t/htm-cheat-sheet/828}
}
@inproceedings{CNN_HTM,
  author    = {Y. {Zou} and Y. {Shi} and Y. {Wang} and Y. {Shu} and Q. {Yuan} and Y. {Tian}},
  booktitle = {2018 IEEE International Conference on Multimedia and Expo (ICME)},
  title     = {Hierarchical Temporal Memory Enhanced One-Shot Distance Learning for Action Recognition},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {1-6},
  doi       = {10.1109/ICME.2018.8486447}
}
  
@article{thousandbrains,
  author   = {Hawkins, Jeff and Lewis, Marcus and Klukas, Mirko and Purdy, Scott and Ahmad, Subutai},
  title    = {A Framework for Intelligence and Cortical Function Based on Grid Cells in the Neocortex},
  journal  = {Frontiers in Neural Circuits},
  volume   = {12},
  pages    = {121},
  year     = {2019},
  url      = {https://www.frontiersin.org/article/10.3389/fncir.2018.00121},
  doi      = {10.3389/fncir.2018.00121},
  issn     = {1662-5110},
  abstract = {How the neocortex works is a mystery. In this paper we propose a novel framework for understanding its function. Grid cells are neurons in the entorhinal cortex that represent the location of an animal in its environment. Recent evidence suggests that grid cell-like neurons may also be present in the neocortex. We propose that grid cells exist throughout the neocortex, in every region and in every cortical column. They define a location-based framework for how the neocortex functions. Whereas grid cells in the entorhinal cortex represent the location of one thing, the body relative to its environment, we propose that cortical grid cells simultaneously represent the location of many things. Cortical columns in somatosensory cortex track the location of tactile features relative to the object being touched and cortical columns in visual cortex track the location of visual features relative to the object being viewed. We propose that mechanisms in the entorhinal cortex and hippocampus that evolved for learning the structure of environments are now used by the neocortex to learn the structure of objects. Having a representation of location in each cortical column suggests mechanisms for how the neocortex represents object compositionality and object behaviors. It leads to the hypothesis that every part of the neocortex learns complete models of objects and that there are many models of each object distributed throughout the neocortex. The similarity of circuitry observed in all cortical regions is strong evidence that even high-level cognitive tasks are learned and represented in a location-based framework.}
}


@misc{fpn,
  title         = {Feature Pyramid Networks for Object Detection},
  author        = {Tsung-Yi Lin and Piotr Dollár and Ross Girshick and Kaiming He and Bharath Hariharan and Serge Belongie},
  year          = {2017},
  eprint        = {1612.03144},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@misc{inceptionnet,
  title         = {Going Deeper with Convolutions},
  author        = {Christian Szegedy and Wei Liu and Yangqing Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and Dumitru Erhan and Vincent Vanhoucke and Andrew Rabinovich},
  year          = {2014},
  eprint        = {1409.4842},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}

@article{htm_neurons,
  author   = {Hawkins, Jeff and Ahmad, Subutai},
  title    = {Why Neurons Have Thousands of Synapses, a Theory of Sequence Memory in Neocortex},
  journal  = {Frontiers in Neural Circuits},
  volume   = {10},
  pages    = {23},
  year     = {2016},
  url      = {https://www.frontiersin.org/article/10.3389/fncir.2016.00023},
  doi      = {10.3389/fncir.2016.00023},
  issn     = {1662-5110},
  abstract = {Pyramidal neurons represent the majority of excitatory neurons in the neocortex. Each pyramidal neuron receives input from thousands of excitatory synapses that are segregated onto dendritic branches. The dendrites themselves are segregated into apical, basal, and proximal integration zones, which have different properties. It is a mystery how pyramidal neurons integrate the input from thousands of synapses, what role the different dendrites play in this integration, and what kind of network behavior this enables in cortical tissue. It has been previously proposed that non-linear properties of dendrites enable cortical neurons to recognize multiple independent patterns. In this paper we extend this idea in multiple ways. First we show that a neuron with several thousand synapses segregated on active dendrites can recognize hundreds of independent patterns of cellular activity even in the presence of large amounts of noise and pattern variation. We then propose a neuron model where patterns detected on proximal dendrites lead to action potentials, defining the classic receptive field of the neuron, and patterns detected on basal and apical dendrites act as predictions by slightly depolarizing the neuron without generating an action potential. By this mechanism, a neuron can predict its activation in hundreds of independent contexts. We then present a network model based on neurons with these properties that learns time-based sequences. The network relies on fast local inhibition to preferentially activate neurons that are slightly depolarized. Through simulation we show that the network scales well and operates robustly over a wide range of parameters as long as the network uses a sparse distributed code of cellular activations. We contrast the properties of the new network model with several other neural network models to illustrate the relative capabilities of each. We conclude that pyramidal neurons with thousands of synapses, active dendrites, and multiple integration zones create a robust and powerful sequence memory. Given the prevalence and similarity of excitatory neurons throughout the neocortex and the importance of sequence memory in inference and behavior, we propose that this form of sequence memory may be a universal property of neocortical tissue.}
}

@article{anomaly_detection,
  title     = {Deep Learning for Anomaly Detection},
  volume    = {54},
  issn      = {1557-7341},
  url       = {http://dx.doi.org/10.1145/3439950},
  doi       = {10.1145/3439950},
  number    = {2},
  journal   = {ACM Computing Surveys},
  publisher = {Association for Computing Machinery (ACM)},
  author    = {Pang, Guansong and Shen, Chunhua and Cao, Longbing and Hengel, Anton Van Den},
  year      = {2021},
  month     = {4},
  pages     = {1–38}
}
@misc{ganomaly,
  title         = {GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training},
  author        = {Samet Akcay and Amir Atapour-Abarghouei and Toby P. Breckon},
  year          = {2018},
  eprint        = {1805.06725},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV}
}
@misc{numenta_example_apps,
  title = {HTM Legacy Applications},
  url   = {https://numenta.com/machine-intelligence-technology/applications/}
}
@misc{htm_rogue,
  title = {Whitepaper: HTM for Rogue Behavior Detection},
  url   = {https://numenta.com/assets/pdf/whitepapers/Rogue%20Behavior%20Detection%20White%20Paper.pdf}
}
@misc{htm_geospatial,
  title = {Whitepaper: HTM for Geospatial Tracking},
  url   = {https://numenta.com/assets/pdf/whitepapers/Geospatial%20Tracking%20White%20Paper.pdf}
}
@misc{htm_finance,
  title = {Github: HTM for Finance},
  url   = {https://github.com/numenta/numenta-apps}
}
@misc{semantic_folding,
  title         = {Semantic Folding Theory And its Application in Semantic Fingerprinting},
  author        = {Francisco De Sousa Webber},
  year          = {2016},
  eprint        = {1511.08855},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI}
}
@article{unknown_detection1,
  title    = {Anomaly Detection with Unknown Anomalies: Application to Maritime Machinery},
  journal  = {IFAC-PapersOnLine},
  volume   = {54},
  number   = {16},
  pages    = {105-111},
  year     = {2021},
  note     = {13th IFAC Conference on Control Applications in Marine Systems, Robotics, and Vehicles CAMS 2021},
  issn     = {2405-8963},
  doi      = {https://doi.org/10.1016/j.ifacol.2021.10.080},
  url      = {https://www.sciencedirect.com/science/article/pii/S2405896321014828},
  author   = {Katarzyna Michałowska and Signe Riemer-Sørensen and Camilla Sterud and Ole Magnus Hjellset},
  keywords = {anomaly detection, predictive maintenance, machine learning, condition-based monitoring, fault detection, diagnosis, grey-box modelling},
  abstract = {We present a framework for deriving anomaly detection algorithms on timeseries data when the time and expression of anomalous behaviour is unknown. The framework is suited for problems in which individual machine learning paradigms cannot be directly implemented: supervised learning is not applicable due to the lack of labelled data, unsupervised learning is not effective since the normal operations are insufficiently defined and take complex and diverse forms, and deep learning risks confusing problematic behaviours for expected ones due to the possible repetitiveness of similar anomalies. The proposed approach is comprised of two phases: unsupervised discovery of anomalies, and semi-supervised construction and tuning of the anomaly detection algorithm. By leveraging data exploration methods and expert knowledge, the resulting algorithms are interpretable and detect a wide range of anomalous behaviours. The approach is applied to the early detection of wear and tear of maritime propulsion and manoeuvring machinery. We show that the final algorithm is able to detect different types of anomalies, including an actual internal leakage in a thruster which is otherwise overlooked by the present rule-based alarm system.}
}
@article{unknown_detection2,
  author  = {Fan, Wei and Miller, Matt and Stolfo, Salvatore and Lee, Wenke and Chan, Philip},
  year    = {2001},
  month   = {10},
  pages   = {},
  title   = {Using Artificial Anomalies to Detect Unknown and Known Network Intrusions},
  volume  = {6},
  journal = {Knowledge and Information Systems},
  doi     = {10.1007/s10115-003-0132-7}
}
@inproceedings{class_imbalance1,
  author = {Devi, Debashree and Biswas, Saroj and Purkayastha, Biswajit},
  year   = {2021},
  month  = {08},
  pages  = {},
  title  = {A Review on Solution to Class Imbalance Problem: Undersampling Approaches}
}
@misc{heterogeneous1,
  title         = {Detecting Anomalies Through Contrast in Heterogeneous Data},
  author        = {Debanjan Datta and Sathappan Muthiah and Naren Ramakrishnan},
  year          = {2021},
  eprint        = {2104.01156},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@inproceedings{autoencoder1,
  title     = {Outlier Detection for Time Series with Recurrent Autoencoder Ensembles},
  author    = {Kieu, Tung and Yang, Bin and Guo, Chenjuan and Jensen, Christian S.},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages     = {2725--2732},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/378},
  url       = {https://doi.org/10.24963/ijcai.2019/378}
}
@article{noise1,
  title    = {Dealing with Noise Problem in Machine Learning Data-sets: A Systematic Review},
  journal  = {Procedia Computer Science},
  volume   = {161},
  pages    = {466-474},
  year     = {2019},
  note     = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
  issn     = {1877-0509},
  doi      = {https://doi.org/10.1016/j.procs.2019.11.146},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050919318575},
  author   = {Shivani Gupta and Atul Gupta},
  keywords = {Noise, Class noise, Attribute noise, Types of noise, Noise identification techniques, Noise handling techniques, Classification},
  abstract = {The occurrences of noisy data in data set can significantly impact prediction of any meaningful information. Many empirical studies have shown that noise in data set dramatically led to decreased classification accuracy and poor prediction results. Therefore, the problem of identifying and handling noise in prediction application has drawn considerable attention over past many years. In our study, we performed a systematic literature review of noise identification and handling studies published in various conferences and journals between January 1993 to July 2018. We have identified 79 primary studies are of noise identification and noise handling techniques. After investigating these studies, we found that among the noise identification schemes, the accuracy of identification of noisy instances by using ensemble-based techniques are better than other techniques. But regarding efficiency, usually single based techniques method is better; it is more suitable for noisy data sets. Among noise handling techniques, polishing techniques generally improve classification accuracy than filtering and robust techniques, but it introduced some errors in the data sets.}
}
@misc{noise2,
  title         = {Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
  author        = {Dan Hendrycks and Thomas Dietterich},
  year          = {2019},
  eprint        = {1903.12261},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@inproceedings{divergentNets,
  title     = {DivergentNets: Medical Image Segmentation by Network Ensemble},
  author    = {Thambawita, Vajira and Hicks, Steven A. and Halvorsen, P{\aa}l and Riegler, Michael A.},
  booktitle = {Proceedings of the 3rd International Workshop and Challenge on Computer Vision in Endoscopy (EndoCV 2021)
               co-located with with the 17th IEEE International Symposium on Biomedical Imaging (ISBI 2021)},
  year      = {2021}
}
@misc{tm_sequence_problem,
  author = {Oleg Iegorov},
  url    = {https://discourse.numenta.org/t/my-analysis-on-why-temporal-memory-prediction-doesnt-work-on-sequential-data/3141},
  title  = {My analysis on why Temporal Memory prediction doesn’t work on sequential data},
  year   = {2017},
  month  = {12}
}
@misc{spiking_neural_networks,
  author = {Rafi, Taki Hasan},
  year   = {2021},
  month  = {04},
  pages  = {},
  title  = {A Brief Review on Spiking Neural Network - A Biological Inspiration},
  doi    = {10.20944/preprints202104.0202.v1}
}
@article{XAI,
  title    = {Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
  journal  = {Information Fusion},
  volume   = {58},
  pages    = {82-115},
  year     = {2020},
  issn     = {1566-2535},
  doi      = {https://doi.org/10.1016/j.inffus.2019.12.012},
  url      = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
  author   = {Alejandro {Barredo Arrieta} and Natalia Díaz-Rodríguez and Javier {Del Ser} and Adrien Bennetot and Siham Tabik and Alberto Barbado and Salvador Garcia and Sergio Gil-Lopez and Daniel Molina and Richard Benjamins and Raja Chatila and Francisco Herrera},
  keywords = {Explainable Artificial Intelligence, Machine Learning, Deep Learning, Data Fusion, Interpretability, Comprehensibility, Transparency, Privacy, Fairness, Accountability, Responsible Artificial Intelligence},
  abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.}
}
@inproceedings{GradCam,
  author    = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  title     = {Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  month     = {Oct},
  year      = {2017}
}
@misc{guided_backprop,
  title         = {Striving for Simplicity: The All Convolutional Net},
  author        = {Jost Tobias Springenberg and Alexey Dosovitskiy and Thomas Brox and Martin Riedmiller},
  year          = {2015},
  eprint        = {1412.6806},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}