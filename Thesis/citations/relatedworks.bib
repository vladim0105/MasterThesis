
@article{perceptron,
  title   = {The perceptron: a probabilistic model for information storage and organization in the brain},
  author  = {Rosenblatt, Frank},
  doi     = {10.1037/h0042519},
  number  = {6},
  volume  = {65},
  month   = {November},
  year    = {1958},
  journal = {Psychological review},
  issn    = {0033-295X},
  pages   = {386—408},
  url     = {https://doi.org/10.1037/h0042519}
}
@article{perceptron2,
  author  = {Freund, Yoav and Schapire, Robert},
  year    = {1999},
  month   = {02},
  pages   = {277-296},
  title   = {Large Margin Classification Using the Perceptron Algorithm},
  volume  = {37},
  journal = {Machine Learning},
  doi     = {10.1023/A:1007662407062},
  url     = {https://doi.org/10.1023/A:1007662407062}
}
@article{perceptron3,
  title   = {Perceptrons},
  author  = {Minsky, Marvin and Papert, Seymour},
  year    = {1969},
  journal = {MIT Press},
  url     = {https://psycnet.apa.org/record/1969-35017-000}
}
@article{perceptron_misconceptions,
  author  = {Mikel Olazaran},
  title   = {A Sociological Study of the Official History of the Perceptrons Controversy},
  journal = {Social Studies of Science},
  volume  = {26},
  number  = {3},
  pages   = {611-659},
  year    = {1996},
  doi     = {10.1177/030631296026003005},
  url     = {https://doi.org/10.1177/030631296026003005}
}
@article{backprop,
  title     = {Learning representations by back-propagating errors},
  author    = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal   = {Nature},
  volume    = {323},
  pages     = {533-536},
  year      = {1986},
  publisher = {Nature Publishing Group},
  url       = {https://doi.org/10.1038/323533a0},
  doi       = {10.1038/323533a0}
}
@misc{relu,
  doi          = {10.48550/ARXIV.1803.08375},
  url          = {https://arxiv.org/abs/1803.08375},
  author       = {Agarap, Abien Fred},
  title        = {Deep Learning using Rectified Linear Units (ReLU)},
  publisher    = {arXiv},
  year         = {2018},
  copyright    = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
  howpublished = {Online}
}
@misc{sgd,
  doi          = {10.48550/ARXIV.1609.04747},
  url          = {https://arxiv.org/abs/1609.04747},
  author       = {Ruder, Sebastian},
  title        = {An overview of gradient descent optimization algorithms},
  publisher    = {arXiv},
  year         = {2016},
  copyright    = {arXiv.org perpetual, non-exclusive license},
  howpublished = {Online}
}
@inproceedings{svm,
  author    = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
  title     = {A Training Algorithm for Optimal Margin Classifiers},
  year      = {1992},
  isbn      = {089791497X},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/130385.130401},
  doi       = {10.1145/130385.130401}
}
@book{kuncheva2019pattern,
  title     = {Pattern Recognition and Neural Networks},
  author    = {Kuncheva, L.},
  isbn      = {9780244232528},
  url       = {https://books.google.no/books?id=A8TKDwAAQBAJ},
  pages     = {157},
  year      = {2019},
  publisher = {Lulu.com}
}

@article{dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}
@inproceedings{batchnorm,
  title     = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author    = {Ioffe, Sergey and Szegedy, Christian},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning (ICML)},
  pages     = {448-456},
  year      = {2015},
  editor    = {Bach, Francis and Blei, David},
  volume    = {37},
  series    = {Proceedings of Machine Learning Research},
  address   = {Lille, France},
  month     = {07-09 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v37/ioffe15.pdf},
  url       = {https://proceedings.mlr.press/v37/ioffe15.html}
}
@article{data_augmentation,
  author  = {Shorten, Connor
             and Khoshgoftaar, Taghi M.},
  title   = {A survey on Image Data Augmentation for Deep Learning},
  journal = {Journal of Big Data},
  year    = {2019},
  month   = {Jul},
  day     = {06},
  volume  = {6},
  number  = {60},
  issn    = {2196-1115},
  doi     = {10.1186/s40537-019-0197-0},
  url     = {https://doi.org/10.1186/s40537-019-0197-0}
}
@inproceedings{autoencoder,
  author    = {Ballard, Dana H.},
  title     = {Modular Learning in Neural Networks},
  year      = {1987},
  isbn      = {0934613427},
  publisher = {AAAI Press},
  booktitle = {Proceedings of the Sixth National Conference on Artificial Intelligence (AAAI)},
  pages     = {279-284},
  numpages  = {6},
  location  = {Seattle, Washington},
  series    = {AAAI'87},
  url       = {https://www.aaai.org/Library/AAAI/1987/aaai87-050.php}
}
@article{lstm,
  author  = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  journal = {Neural Computation},
  title   = {Long Short-Term Memory},
  year    = {1997},
  volume  = {9},
  number  = {8},
  pages   = {1735-1780},
  doi     = {10.1162/neco.1997.9.8.1735},
  url     = {https://doi.org/10.1162/neco.1997.9.8.1735}
}
@misc{gru,
  doi          = {10.48550/ARXIV.1412.3555},
  url          = {https://arxiv.org/abs/1412.3555},
  author       = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  title        = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling},
  publisher    = {arXiv},
  year         = {2014},
  copyright    = {arXiv.org perpetual, non-exclusive license},
  howpublished = {Online}
}
@misc{attention,
  doi          = {10.48550/ARXIV.1409.0473},
  url          = {https://arxiv.org/abs/1409.0473},
  author       = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  title        = {Neural Machine Translation by Jointly Learning to Align and Translate},
  publisher    = {arXiv},
  year         = {2014},
  copyright    = {arXiv.org perpetual, non-exclusive license},
  howpublished = {Online}
}
@inproceedings{transformer,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  booktitle = {Proceedings of Advances in Neural Information Processing Systems (NIPS)},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Attention is All you Need},
  url       = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  volume    = {30},
  year      = {2017}
}
@article{cnn,
  author  = {Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
  journal = {Proceedings of the IEEE},
  title   = {Gradient-based learning applied to document recognition},
  year    = {1998},
  volume  = {86},
  number  = {11},
  pages   = {2278-2324},
  doi     = {10.1109/5.726791},
  url     = {https://doi.org/10.1109/5.726791}
}
@inproceedings{alexnet,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Proceedings of Advances in Neural Information Processing Systems (NIPS)},
  editor    = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  url       = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
  volume    = {25},
  year      = {2012}
}
@inproceedings{deeplearning_gpu,
  author    = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y.},
  title     = {Large-Scale Deep Unsupervised Learning Using Graphics Processors},
  year      = {2009},
  isbn      = {9781605585161},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1553374.1553486},
  doi       = {10.1145/1553374.1553486},
  booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning (ICML)},
  pages     = {873-880},
  numpages  = {8},
  location  = {Montreal, Quebec, Canada},
  series    = {ICML '09}
}
@inproceedings{pytorch,
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Proceedings of Advances in Neural Information Processing Systems (NIPS)},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. Alchee-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  url       = {https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
  volume    = {32},
  year      = {2019}
}
@inproceedings{tensorflow,
  author    = {Martin Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
  title     = {TensorFlow: A System for Large-Scale Machine Learning},
  booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  year      = {2016},
  isbn      = {978-1-931971-33-1},
  address   = {Savannah, GA},
  pages     = {265--283},
  url       = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},
  publisher = {USENIX Association},
  month     = {11}
}
@inproceedings{gan,
  author    = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle = {Proceedings of Advances in Neural Information Processing Systems (NIPS)},
  editor    = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Generative Adversarial Nets},
  url       = {https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
  volume    = {27},
  year      = {2014}
}
@misc{vae,
  doi       = {10.48550/ARXIV.1312.6114},
  url       = {https://arxiv.org/abs/1312.6114},
  author    = {Kingma, Diederik P and Welling, Max},
  title     = {Auto-Encoding Variational Bayes},
  publisher = {arXiv},
  year      = {2013},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{gan_challenges,
  doi       = {10.48550/ARXIV.2005.00065},
  url       = {https://arxiv.org/abs/2005.00065},
  author    = {Saxena, Divya and Cao, Jiannong},
  title     = {Generative Adversarial Networks (GANs): Challenges, Solutions, and Future Directions},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{gan_challenges2,
  doi       = {10.1088/1742-6596/1827/1/012066},
  url       = {https://doi.org/10.1088/1742-6596/1827/1/012066},
  year      = {2021},
  month     = {03},
  publisher = {IOP Publishing},
  volume    = {1827},
  number    = {1},
  pages     = {012066},
  author    = {Haiyang Chen},
  title     = {Challenges and Corresponding Solutions of Generative Adversarial Networks ({GANs}): A Survey Study},
  journal   = {Journal of Physics: Conference Series}
}
@misc{wgan,
  doi       = {10.48550/ARXIV.1701.07875},
  url       = {https://arxiv.org/abs/1701.07875},
  author    = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
  title     = {Wasserstein GAN},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{unrolled_gan,
  doi       = {10.48550/ARXIV.1611.02163},
  url       = {https://arxiv.org/abs/1611.02163},
  author    = {Metz, Luke and Poole, Ben and Pfau, David and Sohl-Dickstein, Jascha},
  title     = {Unrolled Generative Adversarial Networks},
  publisher = {arXiv},
  year      = {2016},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{CycleGAN,
  doi       = {10.48550/ARXIV.1703.10593},
  url       = {https://arxiv.org/abs/1703.10593},
  author    = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  title     = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{cgan,
  doi       = {10.48550/ARXIV.1411.1784},
  url       = {https://arxiv.org/abs/1411.1784},
  author    = {Mirza, Mehdi and Osindero, Simon},
  title     = {Conditional Generative Adversarial Nets},
  publisher = {arXiv},
  year      = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{deeplearning_dataset,
  doi          = {10.48550/ARXIV.1707.02968},
  url          = {https://arxiv.org/abs/1707.02968},
  author       = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
  title        = {Revisiting Unreasonable Effectiveness of Data in Deep Learning Era},
  publisher    = {arXiv},
  year         = {2017},
  copyright    = {arXiv.org perpetual, non-exclusive license},
  howpublished = {Online}
}
@misc{deeplearning_ood,
  doi       = {10.48550/ARXIV.2011.03395},
  url       = {https://arxiv.org/abs/2011.03395},
  author    = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
  title     = {Underspecification Presents Challenges for Credibility in Modern Machine Learning},
  publisher = {arXiv},
  year      = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{memae,
  doi       = {10.48550/ARXIV.1904.02639},
  url       = {https://arxiv.org/abs/1904.02639},
  author    = {Gong, Dong and Liu, Lingqiao and Le, Vuong and Saha, Budhaditya and Mansour, Moussa Reda and Venkatesh, Svetha and Hengel, Anton van den},
  title     = {Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection},
  publisher = {arXiv},
  year      = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{future_frame_prediction,
  doi       = {10.48550/ARXIV.1712.09867},
  url       = {https://arxiv.org/abs/1712.09867},
  author    = {Liu, Wen and Luo, Weixin and Lian, Dongze and Gao, Shenghua},
  title     = {Future Frame Prediction for Anomaly Detection -- A New Baseline},
  publisher = {arXiv},
  year      = {2017},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{smart_surveillance_2,
  author  = {Sreenu, G. and Saleem Durai, M. A.},
  title   = {Intelligent video surveillance: a review through deep learning techniques for crowd analysis},
  journal = {Journal of Big Data},
  year    = {2019},
  month   = {Jun},
  day     = {06},
  volume  = {6},
  number  = {1},
  pages   = {48},
  issn    = {2196-1115},
  doi     = {10.1186/s40537-019-0212-5},
  url     = {https://doi.org/10.1186/s40537-019-0212-5}
}
@misc{adversarial_aes,
  doi       = {10.48550/ARXIV.1511.05644},
  url       = {https://arxiv.org/abs/1511.05644},
  author    = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
  title     = {Adversarial Autoencoders},
  publisher = {arXiv},
  year      = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@unpublished{BAMI,
  title  = {Biological and Machine Intelligence (BAMI)},
  author = {Hawkins, J. and Ahmad, S. and Purdy, S. and Lavin, A.},
  note   = {Initial online release 0.4},
  url    = {https://numenta.com/resources/biological-and-machine-intelligence/},
  year   = {2016}
}
@article{htm_l2_l3,
  author       = {Hawkins, Jeff and Ahmad, Subutai and Cui, Yuwei},
  title        = {Why Does the Neocortex Have Columns, A Theory of Learning the Structure of the World},
  elocation-id = {162263},
  year         = {2017},
  doi          = {10.1101/162263},
  publisher    = {Cold Spring Harbor Laboratory},
  url          = {https://www.biorxiv.org/content/early/2017/09/28/162263},
  journal      = {bioRxiv}
}
@article{AHMAD2017134,
  title   = {Unsupervised real-time anomaly detection for streaming data},
  journal = {Neurocomputing},
  volume  = {262},
  pages   = {134-147},
  year    = {2017},
  note    = {Online Real-Time Learning Strategies for Data Streams},
  issn    = {0925-2312},
  doi     = {https://doi.org/10.1016/j.neucom.2017.04.070},
  url     = {http://www.sciencedirect.com/science/article/pii/S0925231217309864},
  author  = {Subutai Ahmad and Alexander Lavin and Scott Purdy and Zuha Agha}
}
@inbook{ObjectDetectionSIFT,
  author    = {Fallas-Moya, Fabian and Torres-Rojas, Francisco},
  editor    = {Brito-Loeza, Carlos and Espinosa-Romero, Arturo},
  title     = {Object Recognition Using Hierarchical Temporal Memory},
  booktitle = {Intelligent Computing Systems},
  year      = {2018},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {1-14},
  isbn      = {978-3-319-76261-6},
  doi       = {10.1007/978-3-319-76261-6_1},
  url       = {https://doi.org/10.1007/978-3-319-76261-6_1}
}

@inbook{MotionAnomalyDetection,
  author    = {Daylidyonok, Ilya and Frolenkova, Anastasiya and Panov, Aleksandr I.},
  editor    = {Samsonovich, Alexei V.},
  title     = {Extended Hierarchical Temporal Memory for Motion Anomaly Detection},
  booktitle = {Biologically Inspired Cognitive Architectures 2018},
  year      = {2019},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {69-81},
  isbn      = {978-3-319-99316-4},
  doi       = {10.1007/978-3-319-99316-4_10},
  url       = {https://doi.org/10.1007/978-3-319-99316-4_10}
}

@misc{anomalyvideo,
  doi          = {10.48550/ARXIV.2004.00222},
  url          = {https://arxiv.org/abs/2004.00222},
  author       = {Zhu, Sijie and Chen, Chen and Sultani, Waqas},
  title        = {Video Anomaly Detection for Smart Surveillance},
  publisher    = {arXiv},
  year         = {2020},
  copyright    = {arXiv.org perpetual, non-exclusive license},
  howpublished = {Online}
}

@misc{eyeencoder,
  author       = {David McDougall (ctrl-z-9000-times)},
  year         = {2019},
  month        = {9},
  day          = {20},
  url          = {https://github.com/htm-community/htm.core/issues/259#issuecomment-533333336},
  howpublished = {Online}
}
@misc{htm_predictions_count,
  author       = {Sam Heiserman (sheiser1)},
  year         = {2022},
  month        = {1},
  day          = {5},
  howpublished = {Online},
  url          = {https://discourse.numenta.org/t/htm-core-am-i-getting-prediction-density-correctly/9299}
}
@misc{cortical_region,
  author       = {MRaptor},
  year         = {2016},
  month        = {6},
  howpublished = {Online},
  url          = {https://discourse.numenta.org/t/htm-cheat-sheet/828}
}
@inproceedings{CNN_HTM,
  author    = {Y. {Zou} and Y. {Shi} and Y. {Wang} and Y. {Shu} and Q. {Yuan} and Y. {Tian}},
  booktitle = {Proceedings of the 2018 IEEE International Conference on Multimedia and Expo (ICME)},
  title     = {Hierarchical Temporal Memory Enhanced One-Shot Distance Learning for Action Recognition},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {1-6},
  doi       = {10.1109/ICME.2018.8486447},
  url       = {https://doi.org/10.1109/ICME.2018.8486447}
}
  
@article{thousandbrains,
  author   = {Hawkins, Jeff and Lewis, Marcus and Klukas, Mirko and Purdy, Scott and Ahmad, Subutai},
  title    = {A Framework for Intelligence and Cortical Function Based on Grid Cells in the Neocortex},
  journal  = {Frontiers in Neural Circuits},
  volume   = {12},
  pages    = {121},
  year     = {2019},
  url      = {https://www.frontiersin.org/article/10.3389/fncir.2018.00121},
  doi      = {10.3389/fncir.2018.00121},
  issn     = {1662-5110},
  abstract = {How the neocortex works is a mystery. In this paper we propose a novel framework for understanding its function. Grid cells are neurons in the entorhinal cortex that represent the location of an animal in its environment. Recent evidence suggests that grid cell-like neurons may also be present in the neocortex. We propose that grid cells exist throughout the neocortex, in every region and in every cortical column. They define a location-based framework for how the neocortex functions. Whereas grid cells in the entorhinal cortex represent the location of one thing, the body relative to its environment, we propose that cortical grid cells simultaneously represent the location of many things. Cortical columns in somatosensory cortex track the location of tactile features relative to the object being touched and cortical columns in visual cortex track the location of visual features relative to the object being viewed. We propose that mechanisms in the entorhinal cortex and hippocampus that evolved for learning the structure of environments are now used by the neocortex to learn the structure of objects. Having a representation of location in each cortical column suggests mechanisms for how the neocortex represents object compositionality and object behaviors. It leads to the hypothesis that every part of the neocortex learns complete models of objects and that there are many models of each object distributed throughout the neocortex. The similarity of circuitry observed in all cortical regions is strong evidence that even high-level cognitive tasks are learned and represented in a location-based framework.}
}


@misc{fpn,
  doi          = {10.48550/ARXIV.1612.03144},
  url          = {https://arxiv.org/abs/1612.03144},
  author       = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  title        = {Feature Pyramid Networks for Object Detection},
  publisher    = {arXiv},
  year         = {2016},
  howpublished = {Online},
  copyright    = {arXiv.org perpetual, non-exclusive license}
}

@misc{inceptionnet,
  doi          = {10.48550/ARXIV.1409.4842},
  url          = {https://arxiv.org/abs/1409.4842},
  author       = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  title        = {Going Deeper with Convolutions},
  publisher    = {arXiv},
  year         = {2014},
  howpublished = {Online},
  copyright    = {arXiv.org perpetual, non-exclusive license}
}

@article{htm_neurons,
  author   = {Hawkins, Jeff and Ahmad, Subutai},
  title    = {Why Neurons Have Thousands of Synapses, a Theory of Sequence Memory in Neocortex},
  journal  = {Frontiers in Neural Circuits},
  volume   = {10},
  pages    = {23},
  year     = {2016},
  url      = {https://www.frontiersin.org/article/10.3389/fncir.2016.00023},
  doi      = {10.3389/fncir.2016.00023},
  issn     = {1662-5110},
  abstract = {Pyramidal neurons represent the majority of excitatory neurons in the neocortex. Each pyramidal neuron receives input from thousands of excitatory synapses that are segregated onto dendritic branches. The dendrites themselves are segregated into apical, basal, and proximal integration zones, which have different properties. It is a mystery how pyramidal neurons integrate the input from thousands of synapses, what role the different dendrites play in this integration, and what kind of network behavior this enables in cortical tissue. It has been previously proposed that non-linear properties of dendrites enable cortical neurons to recognize multiple independent patterns. In this paper we extend this idea in multiple ways. First we show that a neuron with several thousand synapses segregated on active dendrites can recognize hundreds of independent patterns of cellular activity even in the presence of large amounts of noise and pattern variation. We then propose a neuron model where patterns detected on proximal dendrites lead to action potentials, defining the classic receptive field of the neuron, and patterns detected on basal and apical dendrites act as predictions by slightly depolarizing the neuron without generating an action potential. By this mechanism, a neuron can predict its activation in hundreds of independent contexts. We then present a network model based on neurons with these properties that learns time-based sequences. The network relies on fast local inhibition to preferentially activate neurons that are slightly depolarized. Through simulation we show that the network scales well and operates robustly over a wide range of parameters as long as the network uses a sparse distributed code of cellular activations. We contrast the properties of the new network model with several other neural network models to illustrate the relative capabilities of each. We conclude that pyramidal neurons with thousands of synapses, active dendrites, and multiple integration zones create a robust and powerful sequence memory. Given the prevalence and similarity of excitatory neurons throughout the neocortex and the importance of sequence memory in inference and behavior, we propose that this form of sequence memory may be a universal property of neocortical tissue.}
}

@article{anomaly_detection,
  title     = {Deep Learning for Anomaly Detection},
  volume    = {54},
  issn      = {1557-7341},
  url       = {http://dx.doi.org/10.1145/3439950},
  doi       = {10.1145/3439950},
  number    = {2},
  journal   = {ACM Computing Surveys},
  publisher = {Association for Computing Machinery (ACM)},
  author    = {Pang, Guansong and Shen, Chunhua and Cao, Longbing and Hengel, Anton Van Den},
  year      = {2021},
  month     = {4},
  pages     = {1-38}
}
@misc{ganomaly,
  doi          = {10.48550/ARXIV.1805.06725},
  url          = {https://arxiv.org/abs/1805.06725},
  author       = {Akcay, Samet and Atapour-Abarghouei, Amir and Breckon, Toby P.},
  title        = {GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training},
  publisher    = {arXiv},
  year         = {2018},
  howpublished = {Online},
  copyright    = {arXiv.org perpetual, non-exclusive license}
}
@misc{numenta_example_apps,
  title        = {HTM Legacy Applications},
  howpublished = {Online},
  url          = {https://numenta.com/machine-intelligence-technology/applications/}
}
@misc{htm_rogue,
  title        = {Whitepaper: HTM for Rogue Behavior Detection},
  howpublished = {Online},
  url          = {https://numenta.com/assets/pdf/whitepapers/Rogue%20Behavior%20Detection%20White%20Paper.pdf}
}
@misc{htm_geospatial,
  title        = {Whitepaper: HTM for Geospatial Tracking},
  howpublished = {Online},
  url          = {https://numenta.com/assets/pdf/whitepapers/Geospatial%20Tracking%20White%20Paper.pdf}
}
@misc{htm_finance,
  title        = {Github: HTM for Finance},
  howpublished = {Online},
  url          = {https://github.com/numenta/numenta-apps}
}
@misc{semantic_folding,
  doi          = {10.48550/ARXIV.1511.08855},
  url          = {https://arxiv.org/abs/1511.08855},
  author       = {Webber, Francisco De Sousa},
  title        = {Semantic Folding Theory And its Application in Semantic Fingerprinting},
  publisher    = {arXiv},
  year         = {2015},
  howpublished = {Online},
  copyright    = {arXiv.org perpetual, non-exclusive license}
}
@article{unknown_detection1,
  title    = {Anomaly Detection with Unknown Anomalies: Application to Maritime Machinery},
  journal  = {IFAC-PapersOnLine},
  volume   = {54},
  number   = {16},
  pages    = {105-111},
  year     = {2021},
  note     = {13th IFAC Conference on Control Applications in Marine Systems, Robotics, and Vehicles CAMS 2021},
  issn     = {2405-8963},
  doi      = {https://doi.org/10.1016/j.ifacol.2021.10.080},
  url      = {https://www.sciencedirect.com/science/article/pii/S2405896321014828},
  author   = {Katarzyna Michałowska and Signe Riemer-Sørensen and Camilla Sterud and Ole Magnus Hjellset},
  abstract = {We present a framework for deriving anomaly detection algorithms on timeseries data when the time and expression of anomalous behaviour is unknown. The framework is suited for problems in which individual machine learning paradigms cannot be directly implemented: supervised learning is not applicable due to the lack of labelled data, unsupervised learning is not effective since the normal operations are insufficiently defined and take complex and diverse forms, and deep learning risks confusing problematic behaviours for expected ones due to the possible repetitiveness of similar anomalies. The proposed approach is comprised of two phases: unsupervised discovery of anomalies, and semi-supervised construction and tuning of the anomaly detection algorithm. By leveraging data exploration methods and expert knowledge, the resulting algorithms are interpretable and detect a wide range of anomalous behaviours. The approach is applied to the early detection of wear and tear of maritime propulsion and manoeuvring machinery. We show that the final algorithm is able to detect different types of anomalies, including an actual internal leakage in a thruster which is otherwise overlooked by the present rule-based alarm system.}
}
@article{unknown_detection2,
  author  = {Fan, Wei and Miller, Matt and Stolfo, Salvatore and Lee, Wenke and Chan, Philip},
  year    = {2001},
  month   = {10},
  pages   = {},
  title   = {Using Artificial Anomalies to Detect Unknown and Known Network Intrusions},
  volume  = {6},
  journal = {Knowledge and Information Systems},
  doi     = {10.1007/s10115-003-0132-7},
  url     = {https://doi.org/10.1007/s10115-003-0132-7}
}
@inproceedings{class_imbalance1,
  author    = {Devi, Debashree and Biswas, Saroj K. and Purkayastha, Biswajit},
  booktitle = {Proceedings of the 2020 International Conference on Computational Performance Evaluation (ComPE)},
  title     = {A Review on Solution to Class Imbalance Problem: Undersampling Approaches},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {626-631},
  doi       = {10.1109/ComPE49325.2020.9200087},
  url       = {https://doi.org/10.1109/ComPE49325.2020.9200087}
}
@misc{heterogeneous1,
  doi          = {10.48550/ARXIV.2104.01156},
  url          = {https://arxiv.org/abs/2104.01156},
  author       = {Datta, Debanjan and Muthiah, Sathappan and Ramakrishnan, Naren},
  title        = {Detecting Anomalies Through Contrast in Heterogeneous Data},
  publisher    = {arXiv},
  year         = {2021},
  howpublished = {Online},
  copyright    = {Creative Commons Attribution Share Alike 4.0 International}
}
@inproceedings{autoencoder1,
  title     = {Outlier Detection for Time Series with Recurrent Autoencoder Ensembles},
  author    = {Kieu, Tung and Yang, Bin and Guo, Chenjuan and Jensen, Christian S.},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI)},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages     = {2725--2732},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/378},
  url       = {https://doi.org/10.24963/ijcai.2019/378}
}
@article{noise1,
  title    = {Dealing with Noise Problem in Machine Learning Data-sets: A Systematic Review},
  journal  = {Procedia Computer Science},
  volume   = {161},
  pages    = {466-474},
  year     = {2019},
  note     = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
  issn     = {1877-0509},
  doi      = {https://doi.org/10.1016/j.procs.2019.11.146},
  url      = {https://www.sciencedirect.com/science/article/pii/S1877050919318575},
  author   = {Shivani Gupta and Atul Gupta},
  keywords = {Noise, Class noise, Attribute noise, Types of noise, Noise identification techniques, Noise handling techniques, Classification},
  abstract = {The occurrences of noisy data in data set can significantly impact prediction of any meaningful information. Many empirical studies have shown that noise in data set dramatically led to decreased classification accuracy and poor prediction results. Therefore, the problem of identifying and handling noise in prediction application has drawn considerable attention over past many years. In our study, we performed a systematic literature review of noise identification and handling studies published in various conferences and journals between January 1993 to July 2018. We have identified 79 primary studies are of noise identification and noise handling techniques. After investigating these studies, we found that among the noise identification schemes, the accuracy of identification of noisy instances by using ensemble-based techniques are better than other techniques. But regarding efficiency, usually single based techniques method is better; it is more suitable for noisy data sets. Among noise handling techniques, polishing techniques generally improve classification accuracy than filtering and robust techniques, but it introduced some errors in the data sets.}
}
@misc{noise2,
  doi          = {10.48550/ARXIV.1903.12261},
  url          = {https://arxiv.org/abs/1903.12261},
  author       = {Hendrycks, Dan and Dietterich, Thomas},
  title        = {Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
  publisher    = {arXiv},
  year         = {2019},
  howpublished = {Online},
  copyright    = {arXiv.org perpetual, non-exclusive license}
}
@misc{divergentNets,
  doi          = {10.48550/ARXIV.2107.00283},
  url          = {https://arxiv.org/abs/2107.00283},
  author       = {Thambawita, Vajira and Hicks, Steven A. and Halvorsen, Pål and Riegler, Michael A.},
  title        = {DivergentNets: Medical Image Segmentation by Network Ensemble},
  publisher    = {arXiv},
  year         = {2021},
  howpublished = {Online},
  copyright    = {Creative Commons Attribution 4.0 International}
}
@misc{tm_sequence_problem,
  author       = {Oleg Iegorov},
  url          = {https://discourse.numenta.org/t/my-analysis-on-why-temporal-memory-prediction-doesnt-work-on-sequential-data/3141},
  title        = {My analysis on why Temporal Memory prediction doesn't work on sequential data},
  year         = {2017},
  howpublished = {Online},
  month        = {12}
}
@misc{spiking_neural_networks,
  author       = {Rafi, Taki Hasan},
  year         = {2021},
  month        = {04},
  pages        = {},
  title        = {A Brief Review on Spiking Neural Network - A Biological Inspiration},
  doi          = {10.20944/preprints202104.0202.v1},
  howpublished = {Online},
  url          = {https://doi.org/10.20944/preprints202104.0202.v1}
}
@article{XAI,
  title   = {Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
  journal = {Information Fusion},
  volume  = {58},
  pages   = {82-115},
  year    = {2020},
  issn    = {1566-2535},
  doi     = {https://doi.org/10.1016/j.inffus.2019.12.012},
  url     = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
  author  = {Alejandro {Barredo Arrieta} and Natalia Díaz-Rodríguez and Javier {Del Ser} and Adrien Bennetot and Siham Tabik and Alberto Barbado and Salvador Garcia and Sergio Gil-Lopez and Daniel Molina and Richard Benjamins and Raja Chatila and Francisco Herrera}
}
@misc{GradCamExample,
  title        = {PyTorch library for CAM methods},
  author       = {Jacob Gildenblat and contributors},
  year         = {2021},
  howpublished = {Online},
  publisher    = {GitHub},
  url          = {https://github.com/jacobgil/pytorch-grad-cam},
  howpublished = {Online}
}
@article{GradCam,
  doi       = {10.1007/s11263-019-01228-7},
  url       = {https://doi.org/10.1007%2Fs11263-019-01228-7},
  year      = {2019},
  month     = {10},
  publisher = {Springer Science and Business Media {LLC}},
  volume    = {128},
  number    = {2},
  pages     = {336-359},
  author    = {Ramprasaath R. Selvaraju and Michael Cogswell and Abhishek Das and Ramakrishna Vedantam and Devi Parikh and Dhruv Batra},
  title     = {Grad-{CAM}: Visual Explanations from Deep Networks via Gradient-Based Localization},
  journal   = {International Journal of Computer Vision}
}
@misc{guided_backprop,
  doi          = {10.48550/ARXIV.1412.6806},
  url          = {https://arxiv.org/abs/1412.6806},
  author       = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  title        = {Striving for Simplicity: The All Convolutional Net},
  publisher    = {arXiv},
  year         = {2014},
  howpublished = {Online},
  copyright    = {arXiv.org perpetual, non-exclusive license}
}
@misc{mnist_fashion,
  doi          = {10.48550/ARXIV.1708.07747},
  url          = {https://arxiv.org/abs/1708.07747},
  author       = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  keywords     = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  publisher    = {arXiv},
  year         = {2017},
  howpublished = {Online},
  copyright    = {arXiv.org perpetual, non-exclusive license}
}
@article{exploding_vanishing_gradients,
  author  = {Bengio, Y. and Simard, P. and Frasconi, P.},
  journal = {IEEE Transactions on Neural Networks},
  title   = {Learning long-term dependencies with gradient descent is difficult},
  year    = {1994},
  volume  = {5},
  number  = {2},
  pages   = {157-166},
  doi     = {10.1109/72.279181},
  url     = {https://doi.org/10.1109/72.279181}
}
@inproceedings{htm_medicine,
  author    = {El-Ganainy, Noha O. and Balasingham, Ilangko and Halvorsen, Per Steinar and Arne Rosseland, Leiv},
  booktitle = {Proceedings of the 13th International Symposium on Medical Information and Communication Technology (ISMICT)},
  title     = {On the Performance of Hierarchical Temporal Memory Predictions of Medical Streams in Real Time},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {1-6},
  doi       = {10.1109/ISMICT.2019.8743902},
  url       = {https://doi.org/10.1109/ISMICT.2019.8743902}
}
@article{htm_electrical_load,
  author    = {Osegi, E. N.},
  title     = {Using the hierarchical temporal memory spatial pooler for short-term forecasting of electrical load time series},
  journal   = {Applied Computing and Informatics},
  year      = {2021},
  month     = {Jan},
  day       = {01},
  publisher = {Emerald Publishing Limited},
  volume    = {17},
  number    = {2},
  pages     = {264-278},
  doi       = {10.1016/j.aci.2018.09.002},
  url       = {https://doi.org/10.1016/j.aci.2018.09.002}
}
@article{sift,
  author  = {Lowe, David G.},
  title   = {Distinctive Image Features from Scale-Invariant Keypoints},
  journal = {International Journal of Computer Vision},
  year    = {2004},
  month   = {Nov},
  day     = {01},
  volume  = {60},
  number  = {2},
  pages   = {91-110},
  issn    = {1573-1405},
  doi     = {10.1023/B:VISI.0000029664.99615.94},
  url     = {https://doi.org/10.1023/B:VISI.0000029664.99615.94}
}