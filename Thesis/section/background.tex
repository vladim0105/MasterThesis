\chapter{Background}
The following is relevant background information on HTM theory, Deep Learning, and Video Anomaly Detection.
\section{Deep Learning}
As mentioned in the introduction, deep learning has seen increased popularity for the past few years. Surprisingly enough, the field of deep learning can be traced back to 1958 when the Perceptron\cite{perceptron,perceptron2} was first introduced.
\subsection{Perceptron}
The perceptron\cite{perceptron, perceptron2} was a machine learning algorithm that was based off a simplification of the theory, at the time, on how a neuron worked. The perceptron consists of three parts; the inputs $\mathbf{x}$, the weights $\mathbf{w}$, and the unit step activation function.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{resources/related_works/perceptron.png}
    \caption{Perceptron}
\end{figure}
It also requires a label $y$ corresponding to the input $\mathbf{x}$. The perceptron predicts the label of the input $\mathbf{x}$ to be $\hat{y}=sign(\mathbf{x}\cdot\mathbf{w})$. If $\hat{y}\neq y$ then it updates the weights $\mathbf{w}=\mathbf{w}+y\mathbf{x}$, otherwise it leaves $\mathbf{w}$ unchanged. This is performed until the perceptron reaches convergence, which would happen when all the inputs can be correctly classified. In other words, the perceptron requires that the inputs are linearly separable.
\par
As shown by \textcite{perceptron3}, the perceptron was able to solve linearly separable problems such as the OR function, but was unable to solve the XOR function. For the latter, it was theorized by others that stacking perceptrons in multiple layers, known as a \gls*{g_mlp}, would be able to solve more complex problems such as the aforementioned XOR function\cite{perceptron_misconceptions}. Unfortunately, the work by \textcite{perceptron3} lead to the misconception that \glspl*{g_mlp} would have the same limitations as a single perceptron, and that further progress was impossible which lead to the perceptron and other neuron-based approaches being largely abandoned\cite{perceptron_misconceptions}. This misconception, combined with a lack of computing power and no good approaches to train a \gls*{g_mlp}, lead to a decline in the research of neuron-inspired approaches.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{resources/related_works/mlp.png}
    \caption{Example of a MLP. Source: \cite{mlp_image}}
\end{figure}
\par
It was not until the late 1980s\cite{perceptron_misconceptions} that the \gls*{g_mlp} approach experienced a revival, led by the increase in computational power and the introduction of backpropagation by \textcite{backprop}, which allowed multilayer networks to be easily trained, and gave birth to modern neural networks.
\subsection{Backpropagation}
Backpropagation\cite{backprop}, which is shorthand for "backward propagation of errors",  is a method for efficiently calculating the gradient of the error function, so that it is possible to adjust each individual weight with the purpose of lowering the error. The error function is a function that quantifies the difference between a prediction and the corresponding label, and is differentiable. An example of such an error function, is the \gls*{g_mse}:
\begin{align*}
    MSE=\frac{1}{n}\sum_{i=1}^n (y_i-\hat{y}_i)^2
\end{align*}
The result is that no longer did researchers have to manually engineer features, but could instead apply backpropagation to have the neural network automatically learn internal representations that expressed nontrivial features. An example is that in an image, a collection of pixels could be represented as a line.
\subsection{Neural Networks}
Modern \gls*{g_mlp} networks, often referred to as just "neural networks", are similar to what was theorized earlier. The main difference is that each neuron in a \gls*{g_mlp} can have an arbitrary activation function, such as the sigmoid function and the \gls*{g_relu}\cite{relu} function, which adds nonlinearity into the neural network and allows it to solve complex problems. They also contain more hidden layers (\autoref{fig:nn}), hence the name "deep" learning.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{resources/related_works/nn.png}
    \caption{Example of a neural network. Source: \cite{nn_image}}
    \label{fig:nn}
\end{figure}
The gradients are calculated using backpropagation which are then used by optimization methods to update the weights with the goal of reducing the error. Examples of optimizers are \gls*{g_sgd}\cite{sgd} and AdamW\cite{adamw}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{example-image-a}
    \caption{Gradient Descent}
\end{figure}
\par
Yet with all the advancements within the field of neural networks, it still did not have the popularity that can be observed today. There are several reasons for this, one of them arguably being the introduction of the modern Support Vector Machine (SVM)\cite{svm}, which put neural networks in the background.
\par
The other reason is that neural networks did not have a lot of applications at the time, this being due to a lack of invariances which caused poor performance in machine vision, and no proper way to represent features across time which caused poor performance in time-series analysis. There was also a lack of regularization methods, which caused the models to overfit, and generalize poorly. Additionally, large networks suffered from vanishing or exploding gradients.
\par
In order to improve upon regularization, techniques such as Dropout\cite{dropout}, \gls*{g_bn}\cite{batchnorm}, and data augmentation\cite{data_augmentation} were introduced. New neural network architectures were invented, such as the Autoencoder\cite{autoencoder}, that forces the model to construct an internal representation under constraints, which has a regularizing effect.
\par
As for representing features across time, architectures such as \glspl*{g_rnn}\cite{lstm,gru}, and later attention\cite{attention}-based models such as Transformer\cite{transformer} models were created.\par
The vanishing/exploding gradient problem was alleviated by implementing residual connections\cite{resnet} between layers, among other techniques.
\subsection{Convolutional Neural Networks}
In an attempt to solve the problems regarding invariance and to improve the performance in machine vision tasks, \glspl*{g_cnn} were introduced by \textcite{cnn} in 1998. A \gls*{g_cnn} offers several properties that are useful, such as translational invariance \cite{cnn}. These properties stem from the three architectural ideas in a CNN:
\begin{itemize}
    \item Local receptive fields
    \item Shared weights
    \item Spatial sub-sampling
\end{itemize}
Through the use of data augmentation, it is also possible to not only improve the effectiveness of the aforementioned invariances, but to also introduce a degree of rotational and scale invariance.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{resources/related_works/lenet.jpeg}
    \caption{LeNet\cite{cnn}, an example of a \gls*{g_cnn}.}
\end{figure}
\par
However, the progress within the context of machine vision stagnated due to a lack of computing power when training on large datasets. This stagnation lasted until 2012 when AlexNet was introduced by \textcite{alexnet}, which marked a major turning point in the history of deep learning, as their results proved that deep learning was they way forward for solving advanced problems.
\par
In addition to creating a unique \gls*{g_cnn} architecture, they also made it run on a GPU and made the code publicly available. This was not the first case of running deep learning on a GPU\cite{deeplearning_gpu}, but it can be argued that it was this paper that popularized it.  Using GPUs meant that a vast amount of computational power, for the purpose of training deep learning models, became unlocked. This also paved the way for frameworks such as PyTorch\cite{pytorch} and Tensorflow\cite{tensorflow}, which have democratized deep learning and made it into what it is today.
\subsection{Generative Models}
Some of the latest progress within deep learning can be contributed to generative models, such as \glspl*{g_gan}\cite{gan} and \glspl*{g_vae}\cite{vae}. Generative models work by taking some input, and then generating realistic output. The input could be anything from random noise to a real sample.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{resources/related_works/gan.png}
    \caption{Example of a \gls*{g_gan}.}
\end{figure}
The idea is that the models learn a distribution which describes the data domain represented by the training data, and can then generate synthetic data from that distribution by simply sampling it.
\subsection{Disadvantages}
A disadvantage for deep-learning models in general is that they are susceptible to noise in the dataset \cite{noise1,noise2}. They are also in most cases not self-supervised and therefore require constant tuning in order to stay effective on changing data. Not to mention that they require a lot of data before they can be considered effective. They also suffer from issues with out-of-distribution performance. Susceptible to noise in the data. Also issues with explainability.
\subsubsection{Explainability}
As stated by \textcite{XAI}, as "black-box" approaches such as deep learning surged in popularity, many realized that they offered poor explainability. While it is known \textit{how} the models make their decisions, their huge parametric spaces make it unfeasible to know \textit{why} they decide as they do. Combined with the vast potential that deep learning offers in critical sectors such as medicine, has lead to an increase in focus on developing approaches that offer explainability.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{resources/related_works/gradcam.jpg}
    \caption{Grad-CAM visualization for cat and dog.}
\end{figure}
Approaches such as Grad-CAM \cite{GradCAM} and Guided Backpropagation \cite{guided_backprop} offer improvements in that regard, but these approaches are not made with generative models in mind. In fact, there are very few explainable AI approaches for generative models \cite{XAI}.
\section{Anomaly detection}
As reviewed by \textcite{anomaly_detection}, anomaly detection is often defined as detecting data points that deviate from the general distribution of the data, this also often includes quantifying the level of deviation. Unlike other problems within machine learning and statistics, anomaly detection deals with unpredictable and rare events, therefore adding complexities to problems. Some complexities are as follows:
\begin{itemize}
    \item \textbf{Unknowns} Anomalies are associated with many unknowns which do not become known until the anomaly happens. \cite{unknown_detection1,unknown_detection2} are works that address this.
    \item \textbf{Rarity and class imbalance} Anomalies are by definition rare instances, which means that it becomes difficult to create a balanced dataset. \cite{class_imbalance1} reviews the current solutions to this problem.
    \item \textbf{Heterogeneity} Anomalies can take form in many ways, and as such one class of anomalies can be vastly different from another. Approaches such as \cite{heterogeneous1} have been proposed to alleviate the problem.
\end{itemize}
This makes it hard to apply traditional deep learning methods for anomaly detection, because they are designed with pairs of $\{input, target\}$ in mind.
\subsection{Deep Learning and Anomaly Detection}
The current state-of-the-art algorithms for anomaly detection are numerous, but the main approach is achieved by using deep learning \cite{anomaly_detection}. As previously mentioned, traditional deep learning approaches are hard to apply for anomaly detection. Instead, a popular approach is to use generative deep learning models such as GANs \cite{anomaly_detection, ganomaly,anomalyvideo} to generate synthetic data and compare it to real data in order to detect anomalies. This approach is based on the assumption that the model will only be able to generate data similar to what it has been trained on, and therefore fail when an anomalous event occurs.
\par The advantage is that GANs are generally good at generating realistic data, especially when it comes to images. The disadvantage is that GANs are very hard to train and may give suboptimal results given that it tries to generate good synthetic data rather than directly detect anomalies. The training data also needs to contain all possible non-anomalous classes of events, which may not be a realistic expectation.
\par
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{resources/models/ganomaly.png}
    \caption{GANomaly \cite{ganomaly}, a variation of \gls*{g_gan} for Anomaly detection}
    \label{fig:ganomaly}
\end{figure}
Another common approach is autoencoders, which aim to minimize the reconstruction error from a learned feature representation space \cite{autoencoder1,anomaly_detection,anomalyvideo}. The assumption is that anomalies are more difficult to reconstruct than normal data, hence the reconstruction error will be high and can therefore be used as a metric to detect anomalies.
\par
Additionally, they are very hard to design and train for the purpose of detecting anomalies in complex data such as surveillance. As previously stated, since generative models make up most of the state-of-the-art approaches in anomaly detection, this means that there is a lack of explainability in the field of anomaly detection in videos.
\par
There are also variations of the aforementioned approaches, such as Adversarial Autoencoders, but the core idea is the same; to get an anomaly measure using some sort of generated or reconstructed data.
\subsection{Smart Surveillance}
Smart surveillance, which is the use of automatic video analysis in surveillance, has seen rapid development since its inception. \cite{anomalyvideo} presents and summarizes recent progress for anomaly detection in video for surveillance purposes, where the most promising methods are achieved by using convolutional auto-encoders (AE) and \glspl*{g_gan}. The results show that the deep learning approaches have a high degree of accuracy, and are consistently improving. \par
The paper also discusses problems with using deep learning approaches for anomaly detection. One of the examples that it uses is about a bicycle on campus being wrongly classified as an anomaly, which is related to the aforementioned issue with the non-realistic requirement that the training data must contain all possible classes of non-anomalous events.

\section{Hierarchical Temporal Memory}
Today's machine learning algorithms aim to solve complex problems by simulating a substantial amount of mathematically defined neurons. These neurons are vastly simplified compared to the neurons in the brain and therefore do not have the complexity required to solve complex problems with an accuracy and level of generalizability comparable to the brain. \textcite{BAMI} introduces HTM theory which aims to outline a machine learning algorithm which works on the same principles as the brain and therefore solves some of the aforementioned issues.\par
The brain consists of layers that have been added throughout evolution. The inner layers are responsible for primal intelligence such as hunger, sex and instincts. HTM theory specifically aims to simulate the neocortex which is the outer layer of the brain tasked with advanced logic. It is important to note that HTM only attempts to estimate the activity in the brain, unlike Spiking Neural Networks and others which aim to accurately simulate the activity of the brain \cite{spiking_neural_networks}.
\subsection{Structure}
HTM aims to replicate the structure of the neocortex which is made up of cortical regions. Cortical regions consist of cortical columns, where each column is divided into layers height-wise. These cortical columns are made up of mini-columns, which in turn are made up of neurons.
\begin{figure}[H]
    \centering
    \begin{subfigure}{0.6\textwidth}
        \includegraphics[width=\textwidth]{resources/related_works/cortical_region.png}
        \caption{Cortical Region.}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.35\textwidth}
        \includegraphics[width=\textwidth]{resources/related_works/cortical_column.png}
        \caption{Cortical Column.}
    \end{subfigure}

    \caption{Visualization of the cortical region structure in the neocortex according to HTM theory. Note that current HTM implementations only model L2/L3, but can be extended to model L4 as well \cite{htm_l2_l3}. Source: \cite{cortical_region}}
    \label{fig:cortical_region}
\end{figure}
Neurons in HTM theory are different from neurons in traditional machine learning. The term neuron in traditional machine learning is very misleading and since it is mathematically derived, has actually very little in common with a biological neuron. A biological neuron does not perform back propagation but learns by strengthening and weakening inter-neural connections (synapses), which is something that the HTM neuron attempts to model through Hebbian like learning.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{resources/related_works/neuron_comparison.jpg}
    \caption{Comparison of neurons \protect\cite{htm_neurons}: Traditional Machine Learning (A), Biological Neuron (B), HTM Neuron (C). Source: \cite{BAMI}}
    \label{fig:neuron_comparison}
\end{figure}
The HTM neuron has three inputs \cite{htm_neurons}:
\begin{itemize}
    \item Feedforward, which is the input data
    \item Context, which is data from neighboring neurons and acts as a prediction mechanism for the next feedforward input
    \item Feedback, which is feedback from other neurons in the hierarchy and acts as a prediction mechanism for a sequence of feedforward inputs
\end{itemize}
How this type of neuron operates will be covered in greater detail later in this section.
\subsection{Common Algorithm}
HTM theory states that there is a common algorithm for the intelligence. That the signals from hearing, vision, and touch are at the core processed by the same common algorithm. By extension, this means that HTM networks should be able to solve all kinds of logical tasks.
\subsection{Sparse Distributed Representation}
HTM theory introduces Sparse Distributed Representation (SDR) as a way of representing data in HTM and can be thought of as a bit-array. Each bit theoretically corresponds to a neuron in the neocortex and also represents some semantic information about the current data. This opens up for all kinds of mathematical operations, for instance it is possible to compare the semantic similarities between two SDRs by simply performing a binary AND operation.
\begin{figure}[H]
    \centering
    \includegraphics{resources/related_works/sdr-semantics.png}
    \caption{Semantic similarities between the two SDRs A and B}
    \label{fig:sdr_semantics}
\end{figure}
Observations of the brain has found that at any given point in time, a small percentage of neurons are activated and an SDR aims to keep this property by having a small percentage of bits be 1 at any given point. A common value is 2\% in order to mimic the sparsity of active neurons in the neocortex. Having this property means that the chance of two bit-patterns with different semantic meanings coinciding, for instance due to bit-flips caused by noise in the data, is astronomically low and is what makes HTM robust to noise.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.25\textwidth]{resources/related_works/SDR.png}
    \caption{Example representation of an SDR with a length of 64 and a sparsity of 14.1\%, visualized as a 2D grid.}
\end{figure}
\subsection{Encoders}
To convert real-world data into an SDR, there is a need for an encoder in the pipeline. These encoders can be designed to take potentially any data and convert it into an SDR with an arbitrary sparsity. Given the fact that they may have an arbitrary sparsity, the output SDRs created by the encoder are sometimes referred to as just binary arrays.\par
Writing an encoder is no easy task as it is important to keep semantic similarities between values. This also means that the encoder is perhaps the most important part of an HTM pipeline to get right as it is the part that can limit the system the most.\par

A biological example would be an eye that takes in visual information and converts it into an SDR so that it can be processed by the neocortex. This is the most important part for this thesis as creating a high dimensional encoder for video is still being researched.\par
There are principles that should be followed in order to create a good encoder:
\begin{itemize}
    \item Semantically similar data should result in SDRs with overlapping active bits
    \item The same input should always produce the same SDR
    \item The output must have the same dimensionality (total number of bits) for all inputs
    \item The output should have similar sparsity (similar amount of one-bits) for all inputs and have enough one-bits to handle noise and subsampling
\end{itemize}
As of now, there exists encoders for numbers, categories, geospatial locations, and dates.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.55\linewidth]{resources/related_works/cyclical_encoder.png}
    \caption{Visualization of a cyclical date encoder, which is currently encoding Friday. Note that also Thursday and Saturday are included in the encoding of Friday in order to emphasize that Thursday and Saturday are both equally distanced from Friday. Source: \cite{BAMI}}
    \label{fig:cyclical_encoder}
\end{figure}
Some applications may require anomaly detection on multiple values at once, the correct approach then is to encode the values one by one and then concatenate them into a single SDR before passing it to the HTM.
\par
Several approaches for encoding visual data have been proposed, such as \cite{eyeencoder} which uses a neuroscientifical approach by replicating how the eye works, and \cite{ObjectDetectionSIFT} which uses scale-invariant feature transform (SIFT) to find points of interest in images and encode that information as an SDR. There are also deep learning approaches such as \cite{CNN_HTM} which uses a Convolutional Neural Network (CNN) as part of the encoder, specifically they use the top $n$-features in a feature map as ones and set the rest to zeros in order to construct their SDRs.
\par
The reason for why a direct binary encoding might not perform well is due to the fact that it is neither position nor scale invariant and as such breaks the first principle of creating a good encoder. For instance, if it is desired that two pictures of the same object, but in different scales have more or less the same semantic meaning, then a direct binary encoding is not going to work. Direct binary encodings also lead to loss of information, and is hard to perform for complex objects.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{resources/related_works/resnet18_layer_10.png}
    \caption{Select few feature map activations in the 10th layer of ResNet18\cite{resnet} trained on ImageNet\cite{imagenet}.}
    \label{fig:cnn_features}
\end{figure}
\par
An encoder which transforms convolutional feature maps into SDRs could help solve this, but the issue is converting the dense representation of the feature maps into SDRs. Directly encoding them into SDRs by treating each value in the feature map as a float and converting it into its own SDR quickly becomes intangible due to the processing and memory requirements. \autoref{fig:cnn_features} shows only a small subset of all the feature maps in a single layer, which highlights how intangible this approach is.
\par
Additionally, minor variations in the feature map would cause major variations in the resulting SDR. Alternatively, one could follow \textcite{CNN_HTM} and binary threshold the top-$n$ features, but this leads to its own problems such as loss of information and that the information contained in the top-$n$ features is often undefined in models trained for complex tasks. It is also undefined what the top-$n$ features represent when there are no strong activations.
\subsection{Learning}
Similar to biological beings, HTM is designed to work on streaming data. It does not operate with batches like traditional machine learning, but rather with streaming data that may be changing over time. \par
The learning mechanism consists of two parts; the Spatial Pooler (SP) and the Temporal Memory (TM) algorithm. The latter is also commonly referred to as Sequence Memory. Together they make up the HTM neuron.\par
The spatial pooler takes SDRs produced by the encoder, and uses Hebbian like learning to extract semantically important information into output SDRs. These output SDRs usually have a fixed sparsity of about 2\% due to the fact the spatial pooler aims to produce SDRs that have similar sparsity to what has been observed in the neocortex, but this can be configured at will and is dependent on the problem at hand.\par
The temporal memory algorithm, on the other hand, simulates the learning algorithm in the neocortex. It takes the SDRs formed by the spatial pooler and does two things:
\begin{itemize}
    \item Learns sequences of SDRs formed by the spatial pooler
    \item Forms a prediction, in the form of a predictive SDR,  given the context of previous SDRs
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{resources/related_works/htm_pipeline.png}
    \caption{The HTM Pipeline. A common next-step could be to use a classifier to convert the predictive SDR into a classification. }
    \label{fig:htm_pipeline}
\end{figure}

This gives HTM systems the property of on-line learning, meaning they learn as they go. There is no batch training because each input into the HTM system will update the system. The system effectively builds a predictive model of the data and learns by trying to minimize the error between the true values and the predicted values. This means that the system will continuously adapt to a changing environment.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{resources/related_works/htm_neuron.png}
    \caption{Real neuron and HTM neuron. }
    \label{fig:sp_tm_responsibility}
\end{figure}
\autoref{fig:sp_tm_responsibility} shows that a spatial pooler followed by a temporal memory forms the HTM neuron, where the color green indicates the responsibility of the Spatial Pooler and blue indicates the responsibility of the Temporal Memory.
\subsubsection{Spatial Pooler}
\label{sec:spatial_pooler}
The spatial pooler consists of columns (mini-columns), where each column has a receptive field covering the input. In technical implementations of spatial poolers, the columns exist in name only and could be thought of as nodes instead. A column can cover parts of the input or the entire input, the range being referred to as the \textbf{potential radius}. During initialization, each column creates random connections to a percentage of the bits in the input space within its receptive field, this gives each column a unique \textbf{potential pool} when there are overlaps of receptive fields caused by a large potential radius.\par
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{resources/related_works/sp_vis_alt.png}
    \caption{Visualization of the \gls*{sp} and the potential pool of one of its columns. Source: \cite{htm_sp_presentation}}
\end{figure}
Each connection is described using a \textbf{permanence}-value which can be considered the "strength" of the connection, and it ranges between 0 and 1. During learning, the permanence value of the connections is increased or decreased depending on whether the corresponding bit in the input is active or inactive. When the permanence-value crosses above a \textbf{stimulus threshold}, the connection will be considered "active".

The amount of active connections for a given column is referred to as \textbf{overlap score}. If a column has a high enough overlap score which crosses the \textbf{overlap score threshold}, then the column will itself become active. The reason behind locking activation behind a minimum overlap score is to reduce the influence of noise in the input. Finally, out of all the active columns, only the top $n$ columns with the most overlap score will be selected to be included in the \gls*{sp} output. The value $n$ is chosen so that the \gls*{sp} output has a specific \textbf{sparsity}. Only the selected columns are allowed to learn (increase/decrease permanence). \autoref{fig:sp_vis} visualizes the mechanism that determines the output in the SP.
\par
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{resources/related_works/sp_vis.png}
    \caption{ This figure illustrates how a spatial pooler works. All connections are above the stimulus threshold. The receptive field is 3 bits wide for each column. Overlap score threshold is $1$, and $n=1$. Red means inactive, green means active, and blue means active and selected.}
    \label{fig:sp_vis}
\end{figure}
Because only the active columns are allowed to learn, only a select few columns who got lucky during the random initialization will dominate the spatial pooler output and have a very high \textbf{active duty cycle}. Active duty cycle measures how often a column is active and ranges from 0 (never) to 1 (always). \par
To counter dominating columns, the spatial pooler uses \textbf{boosting}.
The concept behind boosting it to "boost" the overlap score of underperforming columns and lower the overlap score of over performing columns. The result is that more columns learn and contribute to the output, which means that the spatial pooler can then process the input data with a finer granularity. One has to be careful with boosting, since it can cause instability in the spatial pooler output.
\par
It is also possible to have \textbf{topology} in the output by selecting the columns to be included in the output by their local neighborhood, instead of comparing their overlap score globally.
\par
All the aforementioned concepts are configurable in technical implementations.

\subsubsection{Temporal Memory}
\label{sec:temporal_memory}
The temporal memory consists of the columns that a spatial pooler outputs, but treats them as actual columns instead of "nodes". These columns consist of cells and can contain an arbitrary \textbf{number of cells} which defines the capacity of contexts that the temporal memory can express. Each cell in a column can connect to other cells in other columns using segments (more specifically, distal dendrite segments), where each segment consists of synapses connecting to other cells.
\par
Essentially, it takes the "node" based representation of the \gls*{sp} output, and turns it into a new representation which includes state, or context, from previous time steps. It achieves this by only activating a subset of cells per column, typically only one per column. This allows the temporal memory to represent a pattern in multiple contexts. If every column has 32 cells and the \gls*{sp} output has 100 active columns and only one cell per column is active, then the \gls*{tm} has $32^{100}$ ways of representing the same input. The same input will make the same columns active, but in different contexts different cells in those columns will be active.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{resources/related_works/tm_vis_alt2.png}
    \caption{Visualization of the TM, with number of cells equal 4. Some columns are bursting. Source: \cite{BAMI}}
\end{figure}
\par
The temporal memory algorithm consists of two phases. The first phase is to evaluate the \gls*{sp} output against predictions and choose a set of active cells. It does so by looking at the active columns and the cells they contain. If an active column contains predictive cells, then those cells are marked as active. If an active column has no predictive cells, usually caused by observing a new pattern for the first time, then the column "bursts" by activating all the cells that the column contains (see \autoref{fig:tm_vis1}). Otherwise, a cell is inactive.
\par
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{resources/related_works/tm_vis1.png}
    \caption{Expanded \gls*{sp} example with \gls*{tm} component where the number of cells is set to 3. The leftmost column is bursting (all 3 cells activated in green) due to the active \gls*{sp} output and due to containing no predictive cells.}
    \label{fig:tm_vis1}
\end{figure}
At this point, the active cells represent the current input in the context of previous input. For each active column we look at the segments connected to the active cell(s). If the column is bursting we look at the segments that contain any active synapses, if there is no such segment we grow one on the cell with the fewest segments. On each of the segments that we are looking at, we \textbf{increase the permanence} on every active synapse, \textbf{decrease the permanence} on every inactive synapse, and grow new synapses to cells that were previously active. The algorithm also punishes segments that caused cells to enter predictive state, but which did not end up being active.
\par
Since the \gls*{tm} can only grow synapses to cells that were previously active, the \gls*{tm} struggles to express sequences of patterns over multiple timesteps, as has been discussed on HTM forums \cite{tm_sequence_problem}. The solution has been to also encode some temporal information, such as the time of day \cite{AHMAD2017134,tm_sequence_problem}, so that it can use timestamps as anchor points for its contexts.
\par
The second phase is to form a prediction by putting cells into a predictive state. For every segment on every cell, the number of synapses connected to active cells are counted. If the number exceeds an \textbf{activation threshold}, then the segment is marked as active and all the cells connected to the segment enter the predictive state. To summarize, a cell has three possible states:
\begin{itemize}
    \item Active, if the column is bursting or the cell was in a predictive state in the previous time step.
    \item Predictive, if a connected segment is active, which is in turn determined by the amount of active synapses.
    \item Inactive, if none of the other states apply.
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{resources/related_works/tm_vis_alt.png}
    \caption{Visualization of the \gls*{tm} and the three states. Active in black, predictive in gray, and inactive in white. Source: \cite{BAMI}}
\end{figure}
One can configure how much the system can learn by setting the number of cells and the values by which permanence should be increased or decreased. If it is desired that the \gls*{tm} does not "forget" at all, then the permanence value by which synapses are decremented can be set to 0. If it is desired that the \gls*{tm} can only express patterns in the current context and the context of the previous time step, then the number of cells can be set to 2.
\par
Finally, the \gls*{tm} compares the predictions $P_{t-1}$ it made in the previous time step with the actual pattern $A_t$ in the current time step and calculates an anomaly score:
\begin{align*}
    anomalyScore=\frac{|A_t-(P_{t-1}\cap A_t)|}{|A_t|}
\end{align*}
Which is a normalized value from 0 to 1. If the anomaly score is 1, then it means that none of the predicted columns matched the current active columns of the spatial pooler. If it is 0, then it means that all predicted columns matched the current active columns of the SP.\par
It is also possible to estimate the number of predictions being made by the \gls*{tm} at any time \cite{htm_predictions_count}. This is done by counting the number of predictive cells, and dividing them by the number of active bits required to express a pattern. As an example, if sparsity is set so that patterns have 60 active bits and the number of predictive cells is 120, then the estimated number of predictions is given as
\begin{align*}
    numPredictions=\frac{predictiveCells}{activeBits}=\frac{120}{60}=2
\end{align*}
This is only an estimation, in reality the two patterns may have overlapping bits in their representations, and the number of active bits for each representation may have minor deviations.
\subsection{Use cases}
The general use case for HTM is to perform anomaly detection. More specifically, Numenta has made example applications showcasing how HTM can be used in practice \cite{numenta_example_apps}:
\begin{itemize}
    \item \textbf{Rogue Behavior Detection} which models normal behavior and detects anomalies, such as unusual use of files in a network \cite{htm_rogue}.
    \item \textbf{Geospatial Tracking} which detects anomalies in the movement of people, objects, or material, using speed and location data \cite{htm_geospatial}.
    \item \textbf{Financial Monitoring} which detects anomalies in publicly traded companies by continuously modelling stock price, stock volume, and Twitter volume \cite{htm_finance}.
\end{itemize}
There are also applications that are used in production, such as the model offered by \href{www.cortical.io}{cortical.io} which builds upon HTM in order to perform language analysis. They made this possible by introducing Semantic Folding and Semantic Fingerprinting \cite{semantic_folding}.
\subsection{The Thousand Brains Theory}
One of the newest advancements in HTM Theory is the introduction of the Thousand Brains Theory. \textcite{thousandbrains} introduces the Thousand brains theory as a way of redefining hierarchy in the brain based on recent neuroscientifical discoveries. Instead of our classical understanding of hierarchy in deep learning where each layer takes simple features and outputs complex features, we now have that every layer of the hierarchy sees the input at once but at different scales and resolutions. The different nodes in the hierarchy are now also connected and thus enable the network to use all available views of the object in order to create an understanding of that object. \par
To summarize, the object is learned by the brain using multiple models that may rely on different inputs, the models then vote to reach a consensus on what they are sensing. This is coincidentally similar to ensemble learning such as \textcite{divergentNets}. Each model can be thought of as a mini-brain, hence the name "The Thousand Brains Theory".
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.55\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{resources/related_works/hierarchy.png}
        \caption{Classical Hierarchy}
        \label{fig:my_label}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{resources/related_works/thousand_brains.png}
        \caption{Hierarchy as in the Thousand Brains Theory}
        \label{}
    \end{subfigure}
    \caption{Comparison of classical hierarchy and the hierarchy introduced by the Thousand Brains Theory}
\end{figure}
This new type of hierarchy is also quite similar to some state-of-the-art image recognition deep learning architectures such as InceptionNet\cite{inceptionnet} and Feature Pyramid Networks\cite{fpn}, in the sense that they apply different sized convolutional filters, where each filter can be thought of as its own separate model, on the data and do predictions based on all of them at once. This also ensures scale invariance of objects fed in to the architecture.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{resources/models/inception_module.png}
    \caption{How the Inception \cite{inceptionnet} architecture combines multiple filters}
    \label{fig:inception_module}
\end{figure}

While the Thousand Brains Theory is not yet technically implemented in any way in a standard HTM model, it does show that recent developments within deep learning for image analysis have similarities with HTM theory.

\subsection{HTM Performance in Anomaly Detection}
\label{sec:htm_perf}
Knowing that deep learning approaches have a high degree of accuracy but suffer from problems related to generalizability, adaptability, and noise it stands to reason that HTM is a viable alternative for anomaly detection.\par
\textcite{AHMAD2017134} explores the use of HTM for anomaly detection on low dimensional data such as temperature data from an industrial machine. The authors also discuss benchmarks for anomaly detection and compare different methods. The results show that HTM is very capable of performing anomaly detection, especially in a changing environment. HTM is able to outperform other anomaly detection methods and has the advantage of not requiring any per-problem parameter tuning.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{resources/related_works/htm_anomaly_paper_example.jpg}
    \caption{Temperature anomaly detection from \textcite{AHMAD2017134}.}
\end{figure}
For high-dimensional anomaly detection; \textcite{MotionAnomalyDetection} used a HTM system to find anomalous frames in videos of motions (\autoref{fig:motion_frame}). The anomalies were artificially created by swapping certain frames between different motion videos in the dataset. The results show that the HTM system was able to correctly detect some anomalies, but not an impressive amount.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{resources/related_works/motion_frame.png}
    \caption{Example motion frame\cite{MotionAnomalyDetection}.}
    \label{fig:motion_frame}
\end{figure}
One thing to note is that direct binary representations of the video frames were used as SDRs, therefore no proper encoding was performed which might have led to the poor results. This hints at the fact that HTM by itself is not capable of handling high dimensional data, and is instead reliant on an encoder to lower the dimensionality by extracting important spatial features.

\subsection{Deep Learning HTM Encoder}
A potential solution to most of the aforementioned issues could be to combine the deep learning approaches with an HTM network. This way it could be possible to leverage the self-supervision and noise resilience property of HTM, together with the powerful feature extraction and representation of deep learning approaches. Effectively combining the best of both approaches while eliminating the disadvantages that have been previously mentioned.
\section{Summary}